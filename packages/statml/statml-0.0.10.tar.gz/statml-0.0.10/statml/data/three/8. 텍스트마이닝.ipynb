{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영어 텍스트 처리: nltk<a href=\"#영어-텍스트-처리:-nltk\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## tokenize<a href=\"#tokenize\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[1\\]:\n",
    "\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    example_string = \"\"\"\n",
    "    ... Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "    ... And the first lesson of all was the basic trust that he could learn.\n",
    "    ... It's shocking to find how many people do not believe they can learn,\n",
    "    ... and how many more believe learning to be difficult.\"\"\"\n",
    "\n",
    "In \\[3\\]:\n",
    "\n",
    "    # 문장 단위 분할\n",
    "    sent_tokenize(example_string)\n",
    "\n",
    "Out\\[3\\]:\n",
    "\n",
    "    [\"\\nMuad'Dib learned rapidly because his first training was in how to learn.\",\n",
    "     'And the first lesson of all was the basic trust that he could learn.',\n",
    "     \"It's shocking to find how many people do not believe they can learn,\\nand how many more believe learning to be difficult.\"]\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    # 단어 단위 분할\n",
    "    word_tokenize(example_string)[:5]\n",
    "\n",
    "Out\\[5\\]:\n",
    "\n",
    "    [\"Muad'Dib\", 'learned', 'rapidly', 'because', 'his']\n",
    "\n",
    "## filtering stopwords<a href=\"#filtering-stopwords\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    import nltk\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "    [nltk_data] Downloading package stopwords to\n",
    "    [nltk_data]     C:\\Users\\Gilseung\\AppData\\Roaming\\nltk_data...\n",
    "    [nltk_data]   Unzipping corpora\\stopwords.zip.\n",
    "\n",
    "Out\\[7\\]:\n",
    "\n",
    "    True\n",
    "\n",
    "In \\[14\\]:\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    list(stop_words)[:5]\n",
    "\n",
    "Out\\[14\\]:\n",
    "\n",
    "    ['i', 'haven', 'those', 'itself', 'you']\n",
    "\n",
    "## Stemming<a href=\"#Stemming\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[15\\]:\n",
    "\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "In \\[16\\]:\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "In \\[17\\]:\n",
    "\n",
    "    string_for_stemming = \"\"\"\n",
    "    ... The crew of the USS Discovery discovered many discoveries.\n",
    "    ... Discovering is what explorers do.\"\"\"\n",
    "\n",
    "In \\[18\\]:\n",
    "\n",
    "    words = word_tokenize(string_for_stemming)\n",
    "\n",
    "In \\[19\\]:\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "In \\[21\\]:\n",
    "\n",
    "    stemmed_words[:5]\n",
    "\n",
    "Out\\[21\\]:\n",
    "\n",
    "    ['the', 'crew', 'of', 'the', 'uss']\n",
    "\n",
    "## POS<a href=\"#POS\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[23\\]:\n",
    "\n",
    "    nltk.pos_tag(stemmed_words)[:5]\n",
    "\n",
    "Out\\[23\\]:\n",
    "\n",
    "    [('the', 'DT'), ('crew', 'NN'), ('of', 'IN'), ('the', 'DT'), ('uss', 'JJ')]\n",
    "\n",
    "In \\[27\\]:\n",
    "\n",
    "    # 태그 목록 확인\n",
    "    #nltk.help.upenn_tagset()\n",
    "\n",
    "## Lemmatization<a href=\"#Lemmatization\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[35\\]:\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    string_for_lemmatizing = \"The friends of DeSoto love scarves.\"\n",
    "    words = word_tokenize(string_for_lemmatizing)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized_words\n",
    "\n",
    "Out\\[35\\]:\n",
    "\n",
    "    ['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']\n",
    "\n",
    "# 한국어 텍스트 처리: konlpy<a href=\"#한국어-텍스트-처리:-konlpy\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## 데이터 준비<a href=\"#데이터-준비\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[46\\]:\n",
    "\n",
    "    from konlpy.corpus import kolaw\n",
    "    c = kolaw.open('constitution.txt').read()\n",
    "    print(c[:40])\n",
    "\n",
    "    대한민국헌법\n",
    "\n",
    "    유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로\n",
    "\n",
    "## 형태소 분석<a href=\"#형태소-분석\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[34\\]:\n",
    "\n",
    "    from konlpy.tag import *\n",
    "\n",
    "    hannanum = Hannanum()\n",
    "    kkma = Kkma()\n",
    "    komoran = Komoran()\n",
    "    #mecab = Mecab()\n",
    "    #okt = Okt()\n",
    "\n",
    "### 명사 추출<a href=\"#명사-추출\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[48\\]:\n",
    "\n",
    "    print(hannanum.nouns(c[:40]))\n",
    "    print(kkma.nouns(c[:40]))\n",
    "    print(komoran.nouns(c[:40]))\n",
    "\n",
    "    ['대한민국헌법', '유구', '역사', '전통', '빛', '우리', '대한국민', '3·1운동']\n",
    "    ['대한', '대한민국', '대한민국헌법', '민국', '헌법', '유구', '역사', '전통', '우리', '국민', '3', '1', '1운동', '운동']\n",
    "    ['대한민국헌법', '역사', '전통', '한국민', '3·1운동']\n",
    "\n",
    "### 형태소 추출<a href=\"#형태소-추출\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[51\\]:\n",
    "\n",
    "    print(hannanum.morphs(c[:40]))\n",
    "    print(kkma.morphs(c[:40]))\n",
    "\n",
    "    ['대한민국헌법', '유구', '하', 'ㄴ', '역사', '와', '전통', '에', '빛', '나는', '우리', '대한국민', '은', '3·1운동', '으로']\n",
    "    ['대한민국', '헌법', '유구', '하', 'ㄴ', '역사', '와', '전통', '에', '빛나', '는', '우리', '대하', 'ㄴ', '국민', '은', '3', '·', '1', '운동', '으로']\n",
    "\n",
    "### 품사태깅<a href=\"#품사태깅\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[53\\]:\n",
    "\n",
    "    print(hannanum.pos(c[:40]))\n",
    "    print(kkma.pos(c[:40]))\n",
    "\n",
    "    [('대한민국헌법', 'N'), ('유구', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('역사', 'N'), ('와', 'J'), ('전통', 'N'), ('에', 'J'), ('빛', 'N'), ('나는', 'J'), ('우리', 'N'), ('대한국민', 'N'), ('은', 'J'), ('3·1운동', 'N'), ('으로', 'J')]\n",
    "    [('대한민국', 'NNG'), ('헌법', 'NNG'), ('유구', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('역사', 'NNG'), ('와', 'JC'), ('전통', 'NNG'), ('에', 'JKM'), ('빛나', 'VV'), ('는', 'ETD'), ('우리', 'NNM'), ('대하', 'VV'), ('ㄴ', 'ETD'), ('국민', 'NNG'), ('은', 'JX'), ('3', 'NR'), ('·', 'SP'), ('1', 'NR'), ('운동', 'NNG'), ('으로', 'JKM')]\n",
    "\n",
    "## 특수문자 제거<a href=\"#특수문자-제거\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[57\\]:\n",
    "\n",
    "    import re\n",
    "    def clean_text(text):\n",
    "        \"\"\" 한글, 영문, 숫자만 남기고 제거한다. :param text: :return: \"\"\"\n",
    "        text = text.replace(\".\", \" \").strip()\n",
    "        text = text.replace(\"·\", \" \").strip()\n",
    "        pattern = '[^ ㄱ-ㅣ가-힣|0-9|a-zA-Z]+'\n",
    "        text = re.sub(pattern=pattern, repl='', string=text)\n",
    "        return text\n",
    "\n",
    "In \\[58\\]:\n",
    "\n",
    "    clean_text(\"하하하 @@@@ ㅋㅋㅋ !!\")\n",
    "\n",
    "Out\\[58\\]:\n",
    "\n",
    "    '하하하  ㅋㅋㅋ '\n",
    "\n",
    "# Term - document matrix<a href=\"#Term---document-matrix\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## CountVectorizer<a href=\"#CountVectorizer\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "class sklearn.feature_extraction.text.CountVectorizer(\\*,\n",
    "input='content', encoding='utf-8', decode_error='strict',\n",
    "strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None,\n",
    "stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1),\n",
    "analyzer='word', max_df=1.0, min_df=1, max_features=None,\n",
    "vocabulary=None, binary=False, dtype=\\<class 'numpy.int64'>)\n",
    "\n",
    "Parameters\n",
    "\n",
    "-   decode_error: {‘strict’, ‘ignore’, ‘replace’}, default=’strict’\n",
    "-   ngram_range: tuple (min_n, max_n), default=(1, 1)\n",
    "-   binary: bool, default=False (count vs occurence)\n",
    "\n",
    "In \\[30\\]:\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    corpus = ['This is the first document.',\n",
    "              'This document is the second document.',\n",
    "              'And this is the third one.',\n",
    "              'Is this the first document?']\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(X.toarray()) # ndarray\n",
    "\n",
    "    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "    [[0 1 1 1 0 0 1 0 1]\n",
    "     [0 2 0 1 0 1 1 0 1]\n",
    "     [1 0 0 1 1 0 1 1 1]\n",
    "     [0 1 1 1 0 0 1 0 1]]\n",
    "    <class 'numpy.ndarray'>\n",
    "\n",
    "## tf-idf<a href=\"#tf-idf\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[31\\]:\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    corpus = ['This is the first document.',\n",
    "             'This document is the second document.',\n",
    "             'And this is the third one.',\n",
    "             'Is this the first document?']\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(X.toarray())\n",
    "\n",
    "    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "    [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
    "      0.38408524 0.         0.38408524]\n",
    "     [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
    "      0.28108867 0.         0.28108867]\n",
    "     [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
    "      0.26710379 0.51184851 0.26710379]\n",
    "     [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
    "      0.38408524 0.         0.38408524]]\n",
    "\n",
    "# 토픽모델링<a href=\"#토픽모델링\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## 데이터 불러오기<a href=\"#데이터-불러오기\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    import pandas as pd\n",
    "    import gensim\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "     \n",
    "    documents = pd.read_csv('C:/Users/Gilseung/Downloads/news-data.csv/news-data.csv',\n",
    "                            error_bad_lines=False, nrows = 1000)\n",
    "     \n",
    "    documents.head()\n",
    "\n",
    "Out\\[2\\]:\n",
    "\n",
    "|     | publish_date | headline_text                                     |\n",
    "|-----|--------------|---------------------------------------------------|\n",
    "| 0   | 20030219     | aba decides against community broadcasting lic... |\n",
    "| 1   | 20030219     | act fire witnesses must be aware of defamation    |\n",
    "| 2   | 20030219     | a g calls for infrastructure protection summit    |\n",
    "| 3   | 20030219     | air nz staff in aust strike for pay rise          |\n",
    "| 4   | 20030219     | air nz strike to affect australian travellers     |\n",
    "\n",
    "## 데이터 정리<a href=\"#데이터-정리\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    # Use CountVectorizor to find three letter tokens, remove stop_words, \n",
    "    # remove tokens that don't appear in at least 20 documents,\n",
    "    # remove tokens that appear in more than 20% of the documents\n",
    "    vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', \n",
    "                           token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "\n",
    "    # Fit and transform\n",
    "    X = vect.fit_transform(documents.headline_text)\n",
    "\n",
    "    # Convert sparse matrix to gensim corpus.\n",
    "    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "## LDA model 생성<a href=\"#LDA-model-생성\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[12\\]:\n",
    "\n",
    "    # Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
    "    id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n",
    "     \n",
    "    # Use the gensim.models.ldamodel.LdaModel constructor to estimate \n",
    "    # LDA model parameters on the corpus, and save to the variable `ldamodel`\n",
    "    ldamodel = gensim.models.LdaMulticore(corpus=corpus, id2word=id_map, passes=2, num_topics=5, workers=2)\n",
    "\n",
    "## 토픽별 단어 구성 확인<a href=\"#토픽별-단어-구성-확인\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    for idx, topic in ldamodel.print_topics(-1):\n",
    "        print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    Topic: 0 \n",
    "    Words: 0.431*\"war\" + 0.405*\"nsw\" + 0.125*\"iraq\" + 0.006*\"man\" + 0.006*\"police\" + 0.005*\"rain\" + 0.005*\"new\" + 0.005*\"govt\" + 0.005*\"council\" + 0.005*\"court\"\n",
    "\n",
    "\n",
    "    Topic: 1 \n",
    "    Words: 0.417*\"police\" + 0.294*\"iraq\" + 0.245*\"court\" + 0.011*\"rain\" + 0.011*\"govt\" + 0.005*\"man\" + 0.004*\"nsw\" + 0.004*\"new\" + 0.004*\"council\" + 0.004*\"war\"\n",
    "\n",
    "\n",
    "    Topic: 2 \n",
    "    Words: 0.643*\"rain\" + 0.156*\"govt\" + 0.121*\"man\" + 0.032*\"nsw\" + 0.016*\"iraq\" + 0.007*\"police\" + 0.007*\"new\" + 0.006*\"court\" + 0.006*\"war\" + 0.006*\"council\"\n",
    "\n",
    "\n",
    "    Topic: 3 \n",
    "    Words: 0.528*\"man\" + 0.215*\"govt\" + 0.140*\"court\" + 0.078*\"new\" + 0.016*\"police\" + 0.009*\"nsw\" + 0.004*\"iraq\" + 0.004*\"council\" + 0.004*\"rain\" + 0.003*\"war\"\n",
    "\n",
    "\n",
    "    Topic: 4 \n",
    "    Words: 0.469*\"council\" + 0.344*\"new\" + 0.064*\"police\" + 0.060*\"iraq\" + 0.021*\"nsw\" + 0.021*\"rain\" + 0.006*\"man\" + 0.005*\"govt\" + 0.005*\"court\" + 0.005*\"war\"\n",
    "\n",
    "## 토픽 분포 확인<a href=\"#토픽-분포-확인\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[16\\]:\n",
    "\n",
    "    def topic_distribution(string_input):\n",
    "        string_input = [string_input]\n",
    "        # Fit and transform\n",
    "        X = vect.transform(string_input)\n",
    "     \n",
    "        # Convert sparse matrix to gensim corpus.\n",
    "        corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "        output = list(ldamodel[corpus])[0] \n",
    "        return output\n",
    "\n",
    "    # 토픽의 비율: 0번 토픽 - 0.2, 1번 토픽 - 0.2, ...\n",
    "    topic_distribution(documents['headline_text'].iloc[0])\n",
    "\n",
    "Out\\[16\\]:\n",
    "\n",
    "    [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
