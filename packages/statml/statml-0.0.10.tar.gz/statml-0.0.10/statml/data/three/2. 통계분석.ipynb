{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기술 통계<a href=\"#기술-통계\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[1\\]:\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    x = np.random.randint(1, 10, 20)\n",
    "\n",
    "## 중심 통계량: 데이터의 중심경향을 나타내는 수치<a href=\"#중심-통계량:-데이터의-중심경향을-나타내는-수치\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   \\[평균(Average)\\]: 표본데이터의 중심무게 (산술평균, 기하평균,\n",
    "    조화평균, 가중평균)\n",
    "-   \\[중앙값(Median)\\]: 순서를 가진 표본데이터의 가운데(50%)에 위치한 값\n",
    "-   \\[최빈값(Mode)\\]: 표본데이터 중 가장 빈번한 값\n",
    "\n",
    "### 평균<a href=\"#평균\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    np.mean(x)\n",
    "\n",
    "Out\\[2\\]:\n",
    "\n",
    "    5.05\n",
    "\n",
    "### 중앙값<a href=\"#중앙값\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[3\\]:\n",
    "\n",
    "    np.median(x)\n",
    "\n",
    "Out\\[3\\]:\n",
    "\n",
    "    5.0\n",
    "\n",
    "### 최빈값<a href=\"#최빈값\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[4\\]:\n",
    "\n",
    "    stats.mode(x)\n",
    "\n",
    "Out\\[4\\]:\n",
    "\n",
    "    ModeResult(mode=array([1]), count=array([5]))\n",
    "\n",
    "## 변동 통계량: 데이터의 변동성을 나타내는 수치<a href=\"#변동-통계량:-데이터의-변동성을-나타내는-수치\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   범위(Range): 최대값과 최소값의 차이\n",
    "-   편차(Deviation): 관측값과 평균의 차이\n",
    "-   변동(Variation): 편차 제곱의 합\n",
    "-   분산(Variance): 편차 제곱의 합을 데이터의 수로 나눈 값\n",
    "-   표준편차(Standard Deviation): \\$\\\\sqrt{분산}\\$\n",
    "\n",
    "### 범위<a href=\"#범위\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    np.max(x) - np.min(x)\n",
    "\n",
    "Out\\[5\\]:\n",
    "\n",
    "    8\n",
    "\n",
    "### 편차 및 변동<a href=\"#편차-및-변동\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    deviation = x - np.mean(x)\n",
    "    variation = sum(deviation ** 2)\n",
    "\n",
    "### 분산 및 표준편차<a href=\"#분산-및-표준편차\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    np.var(x)\n",
    "\n",
    "Out\\[7\\]:\n",
    "\n",
    "    10.0475\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    np.std(x)\n",
    "\n",
    "Out\\[8\\]:\n",
    "\n",
    "    3.1697791721190924\n",
    "\n",
    "### 사분위수<a href=\"#사분위수\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    np.quantile(x, 0.25)\n",
    "\n",
    "Out\\[9\\]:\n",
    "\n",
    "    1.75\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    np.quantile(x, 0.5)\n",
    "\n",
    "Out\\[10\\]:\n",
    "\n",
    "    5.0\n",
    "\n",
    "## 형태 통계량: 데이터의 분포형태와 왜곡을 나타내는 수치<a href=\"#형태-통계량:-데이터의-분포형태와-왜곡을-나타내는-수치\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   왜도(Skewness): 평균을 중심으로 좌우로 데이터가 편향되어 있는 정도\n",
    "-   첨도(Kurtosis): 뾰족함 정도\n",
    "-   이상치(Outlier): 오류로 판단하는 값이지만 기준이 불명확\n",
    "\n",
    "### 왜도<a href=\"#왜도\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    stats.skew(x)\n",
    "\n",
    "Out\\[11\\]:\n",
    "\n",
    "    -0.017496978366576602\n",
    "\n",
    "### 첨도<a href=\"#첨도\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[12\\]:\n",
    "\n",
    "    stats.kurtosis(x)\n",
    "\n",
    "Out\\[12\\]:\n",
    "\n",
    "    -1.5873447850750733\n",
    "\n",
    "### 아웃라이어 (IQR)<a href=\"#아웃라이어-(IQR)\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    def outlier_detection(x, w = 1.5):\n",
    "        Q1 = np.quantile(x, 0.25)\n",
    "        Q3 = np.quantile(x, 0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        return np.logical_or(Q1 - w * IQR > x, Q3 + w * IQR < x)\n",
    "\n",
    "In \\[14\\]:\n",
    "\n",
    "    x[-1] = 100\n",
    "    outlier_detection(x)\n",
    "\n",
    "Out\\[14\\]:\n",
    "\n",
    "    array([False, False, False, False, False, False, False, False, False,\n",
    "           False, False, False, False, False, False, False, False, False,\n",
    "           False,  True])\n",
    "\n",
    "# 가설 검정<a href=\"#가설-검정\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## 예시 및 정리<a href=\"#예시-및-정리\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "**\\> 이해문제1: 양치기들이 거짓말쟁이인가?**\n",
    "\n",
    "1\\) 가설설정\n",
    "\n",
    "> -   대중주장: 현재 대한민국에 있는 양치기들은 일반인 대비 거짓말을\n",
    ">     많이 하지 않는다!\n",
    "> -   나의주장: 현재 대한민국에 있는 양치기들은 일반인 대비 거짓말을\n",
    ">     많이 한다!\n",
    "\n",
    "2\\) 점추정 및 구간추정\n",
    "\n",
    "> -   검정통계량(점추정):\n",
    ">     \\${샘플집단\\~\\~양치기\\~\\~거짓말\\~\\~빈도\\~\\~-\\~\\~샘플집단\\~\\~일반인\\~\\~거짓말\\~\\~빈도\n",
    ">     \\\\over 샘플집단\\~\\~양치기\\~\\~거짓말\\~\\~빈도\\~\\~표준편차}\\$ (1회성)\n",
    "> -   신뢰구간(구간추정): 실험을 여러번 반복해서\n",
    ">     거짓말차이(검정통계량)의 히스토그램 또는 분포 (반복성)\n",
    "\n",
    "3\\) 유의수준 및 유의확률\n",
    "\n",
    "> -   유의수준: (대중주장이 참인 가정에서, 검정통계량 값으로 나의주장이\n",
    ">     맞다 오판할 확률)  \n",
    ">     : 양치기와 일반인이 거짓말 차이가 없다는 전제에서, 양치기들이\n",
    ">     일반인보다 거짓말 빈도가 많다 오판할 확률\n",
    "> -   유의확률: (대중주장이 참인 가정에서, 검정통계량 값으로 나의주장이\n",
    ">     관찰될 확률)  \n",
    ">     : 양치기와 일반인이 거짓말 차이가 없다는 전제에서, 양치기들이\n",
    ">     일반인보다 거짓말 빈도가 많이 관찰될 확률\n",
    "\n",
    "4\\) 의사결정: (유의수준 5%기준)\n",
    "\n",
    "> -   나의주장 참: 5%보다 작은 경우를 희박한 상황이라고 할때, 나의\n",
    ">     데이터에서 나의주장이 관찰될 확률(3%)은 희박한 결과를 발견하였으니\n",
    ">     양치기들은 거짓말쟁이!\n",
    "> -   대중주장 참: 5%보다 작은 경우를 희박한 상황이라고 할때, 나의\n",
    ">     데이터에서 나의주장이 관찰될 확률(7%)은 희박하지 않은 결과를\n",
    ">     발견한 것이라 양치기들은 거짓말쟁이가 아님!\n",
    "\n",
    "## 등분산 검정<a href=\"#등분산-검정\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "p-value가 낮으면 두 집단 이상의 분산 차이가 존재\n",
    "\n",
    "In \\[15\\]:\n",
    "\n",
    "    from scipy.stats import levene\n",
    "    a = np.random.normal(100, 1, 1000)\n",
    "    b = np.random.normal(100, 1, 1000)\n",
    "    print(np.var(a), np.var(b))\n",
    "\n",
    "    stat, p = levene(a, b)\n",
    "\n",
    "    stat, p\n",
    "\n",
    "    0.9495532722877262 0.9388640228588699\n",
    "\n",
    "Out\\[15\\]:\n",
    "\n",
    "    (0.14584944355485033, 0.7025743724379527)\n",
    "\n",
    "In \\[16\\]:\n",
    "\n",
    "    from scipy.stats import levene\n",
    "    a = np.random.normal(100, 1, 1000)\n",
    "    b = np.random.normal(100, 10, 1000)\n",
    "    print(np.var(a), np.var(b))\n",
    "\n",
    "    stat, p = levene(a, b)\n",
    "    stat, p\n",
    "\n",
    "    1.0119085593907093 102.18348806931758\n",
    "\n",
    "Out\\[16\\]:\n",
    "\n",
    "    (1392.0039029661868, 1.173402793950097e-231)\n",
    "\n",
    "In \\[17\\]:\n",
    "\n",
    "    from scipy.stats import levene\n",
    "    a = np.random.normal(100, 1, 1000)\n",
    "    b = np.random.normal(100, 1, 1000)\n",
    "    c = np.random.normal(100, 10, 1000)\n",
    "    print(np.var(a), np.var(b), np.var(c))\n",
    "\n",
    "    stat, p = levene(a, b, c)\n",
    "    stat, p\n",
    "\n",
    "    0.9073400620556052 0.9886939995274929 94.88913973945444\n",
    "\n",
    "Out\\[17\\]:\n",
    "\n",
    "    (1323.832703547323, 0.0)\n",
    "\n",
    "## 독립 표본 t-test<a href=\"#독립-표본-t-test\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   p-value가 낮으면 두 집단의 평균 차이가 있음\n",
    "-   반드시 등분산 검정 후에 사용할 것\n",
    "\n",
    "scipy.stats.ttest_ind(a, b, axis=0, equal_var=True,\n",
    "nan_policy='propagate', permutations=None, random_state=None,\n",
    "alternative='two-sided', trim=0)\n",
    "\n",
    "-   equal_var: bool, optional\n",
    "\n",
    "          If True (default), perform a standard independent 2 sample test that assumes equal population variances  If False, perform Welch’s t-test, which does not assume equal population variance.\n",
    "\n",
    "-   nan_policy{‘propagate’, ‘raise’, ‘omit’}, optional\n",
    "\n",
    "    -   propagate : returns nan\n",
    "    -   raise : throws an error\n",
    "    -   omit: performs the calculations ignoring nan values\n",
    "\n",
    "In \\[18\\]:\n",
    "\n",
    "    a = np.random.normal(10, 1, 100)\n",
    "    b = np.random.normal(10, 1, 100)\n",
    "\n",
    "    stats.ttest_ind(a, b, equal_var = True)\n",
    "\n",
    "Out\\[18\\]:\n",
    "\n",
    "    Ttest_indResult(statistic=-1.0212913849532985, pvalue=0.30836261766994366)\n",
    "\n",
    "In \\[19\\]:\n",
    "\n",
    "    a = np.random.normal(10, 1, 100)\n",
    "    b = np.random.normal(10, 10, 100)\n",
    "\n",
    "    stats.ttest_ind(a, b, equal_var = False)\n",
    "\n",
    "Out\\[19\\]:\n",
    "\n",
    "    Ttest_indResult(statistic=-0.10412516501227907, pvalue=0.9172757349774978)\n",
    "\n",
    "In \\[20\\]:\n",
    "\n",
    "    a = np.random.normal(10, 1, 100)\n",
    "    b = np.random.normal(15, 1, 100)\n",
    "\n",
    "    stats.ttest_ind(a, b, equal_var = True)\n",
    "\n",
    "Out\\[20\\]:\n",
    "\n",
    "    Ttest_indResult(statistic=-35.174278369876816, pvalue=4.164635776493295e-87)\n",
    "\n",
    "## paired t-test<a href=\"#paired-t-test\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "scipy.stats.ttest_rel(a, b, axis=0, nan_policy='propagate',\n",
    "alternative='two-sided')  \n",
    "Calculate the t-test on TWO RELATED samples of scores, a and b.\n",
    "\n",
    "-   a와 b의 shape가 반드시 일치해야 함\n",
    "\n",
    "In \\[21\\]:\n",
    "\n",
    "    a = np.random.normal(10, 1, 100)\n",
    "    b = a + np.random.normal(0, 1, 100)\n",
    "\n",
    "    stats.ttest_rel(a, b)\n",
    "\n",
    "Out\\[21\\]:\n",
    "\n",
    "    Ttest_relResult(statistic=0.9906563880929291, pvalue=0.3242685134244955)\n",
    "\n",
    "## 정규성 검정<a href=\"#정규성-검정\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   p-value가 0.05 이상인 경우에는 정규분포를 따른다고 봐도 무방함\n",
    "\n",
    "In \\[22\\]:\n",
    "\n",
    "    # 정규 분포를 따르는 경우\n",
    "    from scipy import stats\n",
    "    x = np.random.normal(100, 10, 100)\n",
    "    k2, p = stats.normaltest(x)\n",
    "    print(p)\n",
    "\n",
    "    0.16440012751371216\n",
    "\n",
    "In \\[23\\]:\n",
    "\n",
    "    # 정규 분포를 따르지 않는 경우\n",
    "    from scipy import stats\n",
    "    x = np.random.random(10000)\n",
    "    k2, p = stats.normaltest(x)\n",
    "    print(p)\n",
    "\n",
    "    0.0\n",
    "\n",
    "## 일원분산분석<a href=\"#일원분산분석\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   정규성 검정을 한 뒤에 수행할 것 (정규성 만족못하면 Kruskal-Wallis H\n",
    "    Test 수행)\n",
    "\n",
    "In \\[24\\]:\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import statsmodels.formula.api as smf\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.anova import AnovaRM\n",
    "    from scipy import stats\n",
    "\n",
    "In \\[25\\]:\n",
    "\n",
    "    # 데이터 준비\n",
    "    # information on experimental design\n",
    "    group_list = ['control','patient1','patient2']\n",
    "    subs_list = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10']\n",
    "\n",
    "    # read data into dataframe\n",
    "    df_1way = pd.DataFrame(columns=[\"group\", \"my_value\"])\n",
    "    my_row = 0\n",
    "    for ind_g, group in enumerate(group_list):\n",
    "        for sub in subs_list:\n",
    "            # generate random value here as example\n",
    "            my_val = np.random.normal(ind_g, 1, 1)[0]\n",
    "            df_1way.loc[my_row] = [group, my_val]\n",
    "            my_row = my_row + 1\n",
    "\n",
    "In \\[26\\]:\n",
    "\n",
    "    df_1way.head()\n",
    "\n",
    "Out\\[26\\]:\n",
    "\n",
    "|     | group   | my_value  |\n",
    "|-----|---------|-----------|\n",
    "| 0   | control | 0.907117  |\n",
    "| 1   | control | 0.471981  |\n",
    "| 2   | control | 0.148081  |\n",
    "| 3   | control | 0.318889  |\n",
    "| 4   | control | -0.049459 |\n",
    "\n",
    "### statsmodel을 활용<a href=\"#statsmodel을-활용\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[27\\]:\n",
    "\n",
    "    # generate model for linear regression\n",
    "    my_model = smf.ols(formula='my_value ~ group', data=df_1way)\n",
    "\n",
    "    # fit model to data to obtain parameter estimates\n",
    "    my_model_fit = my_model.fit()\n",
    "\n",
    "    # show anova table\n",
    "    anova_table = sm.stats.anova_lm(my_model_fit, typ=2)\n",
    "    print(anova_table)\n",
    "\n",
    "    # group - F, PR (p-value)만 확인하면 됨\n",
    "\n",
    "                 sum_sq    df          F    PR(>F)\n",
    "    group     19.105558   2.0  15.193014  0.000038\n",
    "    Residual  16.976555  27.0        NaN       NaN\n",
    "\n",
    "### scipy stats을 활용<a href=\"#scipy-stats을-활용\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[28\\]:\n",
    "\n",
    "    F, p = stats.f_oneway(df_1way[df_1way['group'] == 'control'].my_value, df_1way[df_1way['group'] == 'patient1'].my_value, df_1way[df_1way['group'] == 'patient2'].my_value)\n",
    "    print(F, p)\n",
    "\n",
    "    15.193013869948365 3.797760762024205e-05\n",
    "\n",
    "## 이원분산분석 & 교호작용분석<a href=\"#이원분산분석-&amp;-교호작용분석\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[29\\]:\n",
    "\n",
    "    # information on experimental design\n",
    "    group_list = ['control','patient1','patient2']\n",
    "    language_list = ['English', 'German', 'French']\n",
    "    subs_list = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10']\n",
    "\n",
    "    # read data into dataframe\n",
    "    df_2way = pd.DataFrame(columns=[\"group\", \"language\", \"my_value\"])\n",
    "    my_row = 0\n",
    "    for ind_g, group in enumerate(group_list):\n",
    "        for ind_l, lan in enumerate(language_list):\n",
    "            for sub in subs_list:\n",
    "                    # generate random value here as example\n",
    "                    my_val = np.random.normal(ind_g + ind_l, 1, 1)[0]\n",
    "                    df_2way.loc[my_row] = [group, lan, my_val]\n",
    "                    my_row = my_row + 1\n",
    "\n",
    "    df_2way.head()\n",
    "\n",
    "Out\\[29\\]:\n",
    "\n",
    "|     | group   | language | my_value  |\n",
    "|-----|---------|----------|-----------|\n",
    "| 0   | control | English  | 0.246734  |\n",
    "| 1   | control | English  | 1.052874  |\n",
    "| 2   | control | English  | -0.474547 |\n",
    "| 3   | control | English  | -0.769478 |\n",
    "| 4   | control | English  | -0.372473 |\n",
    "\n",
    "In \\[30\\]:\n",
    "\n",
    "    # fit model to data to obtain parameter estimates\n",
    "    # formula = Y ~ 인자 * 인자\n",
    "    my_model_fit = smf.ols(formula='my_value ~ group * language', data=df_2way).fit()\n",
    "\n",
    "\n",
    "    # show anova table\n",
    "    print(sm.stats.anova_lm(my_model_fit, typ=2))\n",
    "\n",
    "    # group, language은 유효\n",
    "    # group*language의 p-value는 0.25로 유효하지 않음 --> 두 독립 변수의 교호작용은 없음!\n",
    "\n",
    "                       sum_sq    df          F        PR(>F)\n",
    "    group           82.481682   2.0  42.847287  2.022200e-13\n",
    "    language        49.828294   2.0  25.884623  2.033270e-09\n",
    "    group:language   9.053971   4.0   2.351662  6.091095e-02\n",
    "    Residual        77.963119  81.0        NaN           NaN\n",
    "\n",
    "## kruskal test (정규성 검정 실패시, 사용하는 One-way ANOVA)<a href=\"#kruskal-test-(정규성-검정-실패시,-사용하는-One-way-ANOVA)\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "scipy.stats.kruskal(sample1,sample2,...)\n",
    "\n",
    "-   p value가 작을수록 차이가 있음\n",
    "\n",
    "In \\[31\\]:\n",
    "\n",
    "    from scipy import stats\n",
    "    x = [1, 3, 5, 7, 9]\n",
    "    y = [2, 4, 6, 8, 10]\n",
    "    stats.kruskal(x, y)\n",
    "\n",
    "Out\\[31\\]:\n",
    "\n",
    "    KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n",
    "\n",
    "In \\[32\\]:\n",
    "\n",
    "    from scipy import stats\n",
    "    x = [1,2,3,4,5]\n",
    "    y = [1,2,3,4,5]\n",
    "    z = [1,2,3,4,5]\n",
    "    stats.kruskal(x, y,z)\n",
    "\n",
    "Out\\[32\\]:\n",
    "\n",
    "    KruskalResult(statistic=0.0, pvalue=1.0)\n",
    "\n",
    "## 교차 분석<a href=\"#교차-분석\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "p가 작을수록 독립\n",
    "\n",
    "In \\[33\\]:\n",
    "\n",
    "    from scipy.stats import chi2_contingency\n",
    "    df = pd.DataFrame({'Gender' : ['M', 'M', 'M', 'F', 'F'] * 10,\n",
    "                       'isSmoker' : ['Smoker', 'Smoker', 'Smoker', 'Non-Smoker', 'Non-Smoker'] * 10\n",
    "                      })\n",
    "    df.head()\n",
    "\n",
    "Out\\[33\\]:\n",
    "\n",
    "|     | Gender | isSmoker   |\n",
    "|-----|--------|------------|\n",
    "| 0   | M      | Smoker     |\n",
    "| 1   | M      | Smoker     |\n",
    "| 2   | M      | Smoker     |\n",
    "| 3   | F      | Non-Smoker |\n",
    "| 4   | F      | Non-Smoker |\n",
    "\n",
    "In \\[34\\]:\n",
    "\n",
    "    contigency= pd.crosstab(df['Gender'], df['isSmoker']) \n",
    "    contigency\n",
    "\n",
    "Out\\[34\\]:\n",
    "\n",
    "| isSmoker | Non-Smoker | Smoker |\n",
    "|----------|------------|--------|\n",
    "| Gender   |            |        |\n",
    "| F        | 20         | 0      |\n",
    "| M        | 0          | 30     |\n",
    "\n",
    "In \\[35\\]:\n",
    "\n",
    "    # Chi-square test of independence. \n",
    "    c, p, dof, expected = chi2_contingency(contigency) \n",
    "    # Print the p-value\n",
    "    print(p)\n",
    "\n",
    "    1.2317319065658562e-11\n",
    "\n",
    "## 상관분석<a href=\"#상관분석\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[36\\]:\n",
    "\n",
    "    from scipy.stats import pearsonr\n",
    "    from scipy.stats import spearmanr\n",
    "\n",
    "    x1 = np.random.random(100)\n",
    "    x2 = np.random.random(100)\n",
    "\n",
    "    print(pearsonr(x1, x2))\n",
    "    print(spearmanr(x1, x2))\n",
    "\n",
    "    (-0.009913368485027201, 0.922019862002112)\n",
    "    SpearmanrResult(correlation=-0.016393639363936393, pvalue=0.8713954941535211)\n",
    "\n",
    "## 요인분석 및 주성분분석<a href=\"#요인분석-및-주성분분석\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "sklearn.decomposition.FactorAnalysis(n_components=None, \\*, tol=0.01,\n",
    "copy=True, max_iter=1000, noise_variance_init=None,\n",
    "svd_method='randomized', iterated_power=3, rotation=None,\n",
    "random_state=0)\n",
    "\n",
    "sklearn.decomposition.PCA(n_components=None, \\*, copy=True,\n",
    "whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto',\n",
    "random_state=None)\n",
    "\n",
    "# 회귀분석<a href=\"#회귀분석\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## 데이터 준비<a href=\"#데이터-준비\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[37\\]:\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    X = pd.DataFrame(np.random.random((100, 5)), columns = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"])\n",
    "    Y = X[\"X1\"] + X[\"X2\"] + np.random.random(100) / 10\n",
    "\n",
    "## 회귀모델 기초<a href=\"#회귀모델-기초\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "### 모델링<a href=\"#모델링\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[38\\]:\n",
    "\n",
    "    X = sm.add_constant(X) # adding a constant (1로만 구성된 변수가 추가됨)\n",
    "\n",
    "    model = sm.OLS(Y, X).fit() # Y, X 순서임을 확인\n",
    "    predictions = model.predict(X)  # X에 대한 예측치 (Series)\n",
    "\n",
    "    print_model = model.summary()\n",
    "    print(print_model)\n",
    "\n",
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   R-squared:                       0.995\n",
    "    Model:                            OLS   Adj. R-squared:                  0.995\n",
    "    Method:                 Least Squares   F-statistic:                     3816.\n",
    "    Date:                Thu, 19 Aug 2021   Prob (F-statistic):          7.06e-107\n",
    "    Time:                        14:34:29   Log-Likelihood:                 217.82\n",
    "    No. Observations:                 100   AIC:                            -423.6\n",
    "    Df Residuals:                      94   BIC:                            -408.0\n",
    "    Df Model:                           5                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    const          0.0437      0.013      3.425      0.001       0.018       0.069\n",
    "    X1             0.9952      0.010     98.338      0.000       0.975       1.015\n",
    "    X2             1.0091      0.011     91.522      0.000       0.987       1.031\n",
    "    X3            -0.0045      0.010     -0.438      0.663      -0.025       0.016\n",
    "    X4             0.0084      0.010      0.799      0.426      -0.012       0.029\n",
    "    X5             0.0061      0.010      0.613      0.541      -0.014       0.026\n",
    "    ==============================================================================\n",
    "    Omnibus:                       27.854   Durbin-Watson:                   2.298\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):                5.636\n",
    "    Skew:                          -0.051   Prob(JB):                       0.0597\n",
    "    Kurtosis:                       1.841   Cond. No.                         9.08\n",
    "    ==============================================================================\n",
    "\n",
    "    Notes:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "\n",
    "### 해석 필요<a href=\"#해석-필요\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   Standard error가 가지는 의미: The standard error is an estimate of\n",
    "    the standard deviation of the coefficient, the amount it varies\n",
    "    across cases. It can be thought of as a measure of the precision\n",
    "    with which the regression coefficient is measured.\n",
    "-   P>\\|t\\|: p-value (0.05미만이면 유의)\n",
    "-   \\[0.025 0.975\\]: 신뢰 구간\n",
    "-   Prob (F-statistic): the probability that the null hypothesis for the\n",
    "    full model is true (i.e., that all of the regression coefficients\n",
    "    are zero)\n",
    "\n",
    "## 회귀모델 가정 검토<a href=\"#회귀모델-가정-검토\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "### 선형성<a href=\"#선형성\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   응답 변수가 예측 변수와 선형 회귀 계수의 선형 조합으로 표현 가능함을\n",
    "    의미\n",
    "-   상관계수, scatter plot으로 검정\n",
    "\n",
    "#### scatter plot<a href=\"#scatter-plot\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[39\\]:\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    fig = plt.figure(figsize = (6, 10))\n",
    "    for i in range(1, 6):\n",
    "        plt.subplot(3, 2, i)\n",
    "        plt.title(\"X{}~Y\".format(i))\n",
    "        plt.scatter(X[\"X{}\".format(i)], Y)\n",
    "    fig.tight_layout()\n",
    "\n",
    "#### 상관계수<a href=\"#상관계수\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[40\\]:\n",
    "\n",
    "    from scipy import stats\n",
    "    for i in range(1, 6):\n",
    "        print(i, stats.pearsonr(X[\"X\" + str(i)], Y)[0])\n",
    "\n",
    "    1 0.7169488074770946\n",
    "    2 0.6999618891342666\n",
    "    3 -0.1159727038320417\n",
    "    4 0.12254363846193096\n",
    "    5 -0.09733277938669578\n",
    "\n",
    "### 독립성<a href=\"#독립성\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   다중공선성이 없어야 함\n",
    "-   다중공선성을 일으키는 변수 (VIF 10이상)를 제거하거나, 변수선택법을\n",
    "    이용하여 해결\n",
    "\n",
    "In \\[41\\]:\n",
    "\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    pd.Series([variance_inflation_factor(X.values, i) \n",
    "               for i in range(X.shape[1])], \n",
    "              index=X.columns)\n",
    "\n",
    "Out\\[41\\]:\n",
    "\n",
    "    const    20.407675\n",
    "    X1        1.002517\n",
    "    X2        1.097224\n",
    "    X3        1.042954\n",
    "    X4        1.057066\n",
    "    X5        1.036779\n",
    "    dtype: float64\n",
    "\n",
    "### 잔차 등분산성<a href=\"#잔차-등분산성\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "#### 잔차 그림<a href=\"#잔차-그림\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[42\\]:\n",
    "\n",
    "    # 잔차가 -0.06부터 0.06까지 고르게 퍼져 있음 \n",
    "    # 정상적인 잔차그림은 0을 중심으로 에측 값에 관계없이 일정 범위 내에서 특정한 패턴을 가지지 않게 분포됩니다.\n",
    "    res = model.resid\n",
    "    plt.scatter(range(len(res)), res.values)\n",
    "\n",
    "Out\\[42\\]:\n",
    "\n",
    "    <matplotlib.collections.PathCollection at 0x17f1cf60c88>\n",
    "\n",
    "![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAdW0lEQVR4nO3df7Bc5X3f8fcnAhLZSUZQsC2uRCS3Co4SJ4beAKnajCNMQdhjMW6ngYxr6nZGQ2tS25PgXIa/+pc1JZMaTymMgkmh8RgYm2KNrYmCIW2nzEC4MoQYhIKKHetKspHTQjKBCcb+9o89N1ktu/eevefsOc85z+c1c+fePXv27vPs7nm+z+9VRGBmZvn6kbYTYGZm7XIgMDPLnAOBmVnmHAjMzDLnQGBmlrkz2k7AWpx77rmxZcuWtpNhZtYphw4d+l5EnDd6vJOBYMuWLSwuLradDDOzTpH05+OOu2vIzCxzDgRmZplzIDAzy5wDgZlZ5hwIzMwy50BgZpY5BwIzs8w5EJiZZc6BwMwscw4EZmaZcyAwM8ucA4GZWeYcCMzMMudAYGaWOQcCM7PM1RIIJF0l6Yiko5IWxtwvSZ8t7n9G0sVD922Q9EVJz0s6LOmX6kiTmZmVUzkQSFoH3A7sArYD10naPnLaLmBb8bMHuGPovtuAP4iIdwG/AByumiYzMyuvjhbBJcDRiHgxIl4H7gN2j5yzG7g3Bh4HNkjaKOkngV8GPgcQEa9HxMs1pMnMzEqqIxDMAceGbi8Vx8qc807gFPB7kp6SdJekt457Ekl7JC1KWjx16lQNyTYzM6gnEGjMsSh5zhnAxcAdEXER8NfAm8YYACJiX0TMR8T8eee96buXzcxsjeoIBEvA5qHbm4ATJc9ZApYi4oni+BcZBAYzM2tIHYHgSWCbpK2SzgKuBfaPnLMf+Egxe+gy4JWIOBkR3wGOSbqwOO9y4Lka0mRmZiWdUfUfRMQbkm4EDgLrgLsj4llJNxT33wkcAK4GjgKvAh8d+he/Dny+CCIvjtxnZmYzpojR7vz0zc/Px+LiYtvJMDPrFEmHImJ+9LhXFpuZZc6BwMwsc5XHCMwsbw89dZxbDx7hxMuvcf6G9dx05YVcc9HoUiJLmQOBma3ZQ08d5+YH/5TXvv8DAI6//Bo3P/inAA4GHeKuITNbs1sPHvnbILDste//gFsPHmkpRbYWDgRmtmYnXn5tquOWJgcCM1uz8zesn+q4pcmBoKSHnjrOjr2PsnXhq+zY+ygPPXW87SSZte6mKy9k/ZnrTju2/sx13HTlhRMeYSnyYHEJHhAzG2/58+9ZQ93mQFDCSgNi/sBb7q65aM7XQcc5EJTgATHrC8/5t3E8RlCCB8SsD5a7OI+//BrB33VxerzLHAhK8ICY9YHn/Nsk7hoqwQNi1gfu4rRJHAhK8oCYdd35G9ZzfEyh7y5Oc9eQWSbcxWmTuEVglgl3cdokDgRmK+jbdEt3cdo4DgRmE3hFueXCYwRmE3i6peUi+xZBik3/FNOUI0+3tFzU0iKQdJWkI5KOSloYc78kfba4/xlJF4/cv07SU5K+Ukd6ykpxpWWKacqVV5RbLioHAknrgNuBXcB24DpJ20dO2wVsK372AHeM3P9x4HDVtEwrxaZ/imnKVdXpln3eurzPectRHS2CS4CjEfFiRLwO3AfsHjlnN3BvDDwObJC0EUDSJuD9wF01pGUqKTb9U0xTrq65aI5Pf+jdzG1Yj4C5Dev59IfeXaqbrs8tuz7nLVd1jBHMAceGbi8Bl5Y4Zw44CXwG+BTwEys9iaQ9DFoTXHDBBdVSXEhxpWWKaVpJ38cz1jrdss9bl/c5b7mqo0WgMceizDmSPgC8FBGHVnuSiNgXEfMRMX/eeeetJZ1vkuJKyxTTNIlrhpP1uWXX57zlqo5AsARsHrq9CThR8pwdwAclfYtBl9JOSb9fQ5pKqdL0zylNk3g8Y7I+DzT3OW+5qqNr6Elgm6StwHHgWuDXRs7ZD9wo6T4G3UavRMRJ4ObiB0nvBX4zIj5cQ5pKS3GlZYppGsc1w8luuvLC0xajQbotu2n1OW+5qhwIIuINSTcCB4F1wN0R8aykG4r77wQOAFcDR4FXgY9WfV5rX9fGM5o0q319UhiT8Z5Fb5bC+1KFIka789M3Pz8fi4uLbScje6NbMMCgZphqV1bX+fVOU5feF0mHImJ+9Li3mLA169J4Rh94TCZNfXhfst9iwqrpynhGH3hMJk19eF/cIjDrCM/WSVMf3hcHArNErLZtQ5fWmOSkD++Lu4bMElDmuw88W2cgtRk6Vd+XSflpMp+eNWSWgB17Hx07FXduw3oeW9jZQorS1KUZOmVMys8/+4dzfOnQ8drzOWnWkFsEZmswbW1ttfP7MODYhL7tczQpP1944hg/GKmkzzKfDgRmU5r2KyzLnO/FeeX0LWBOSvdoEFjt/Ko8WGw2pWnnjZc5vw8Djk3owwydYZPSvU7j9umcXT4dCMymNG2ttMxxL84rZy0BM+Uv0ZmUn+su3dxoxcBdQ9YLTc6wmLYbp+z5uS7Om+a9m3aGzrTdeE1bKT/zP3WOZw2txLOG0rfSxV13od30TJJpn69vM13qNOvXxrOxTudZQ0NSm4fcNyvVwoDaa2hNzySZtlbq+f+Tzfq969vg8qxkFwhSbyquVUrBbbXB0bov/DYu9mm7cXLt9lnNrN87z8YqJ7vB4j7sFDgqta+MXOninsWF37eZJDmZ9Xvn2VjlZBcI+thUTC24rXRxz+LC98XeXbN+7zwbq5zsuobqbiqm0CWTWnBb7asM6/6aQ/fBd1cT712d3XIpXO+zkF0gqPP7VlMZb0itH7TMxV3HxdTXizI3XRk/SeV6n4Usp4/WVYCkMjWt7BS8PhWcnpJpTat6vadw/Xn66JC6aiCpdMmUqYH3rTbTt83HLH1VrvfUr79aAoGkq4DbgHXAXRGxd+R+FfdfDbwK/KuI+LqkzcC9wDuAHwL7IuK2OtLUhJS6ZFYLbn0rOFMJwpaPKtd76tdf5VlDktYBtwO7gO3AdZK2j5y2C9hW/OwB7iiOvwH8RkT8DHAZ8LExj23MtHuSdGm2St8KTk8ZtaZVud5Tv/7qmD56CXA0Il6MiNeB+4DdI+fsBu6NgceBDZI2RsTJiPg6QET8FXAYaCU8rmUufpempvWt4OxSELZ+qHK9p3791dE1NAccG7q9BFxa4pw54OTyAUlbgIuAJ2pI09TW2nTryoyHcbOlxCDg7dj7aGsDx2sdQPOU0W5LYeB0LdZ6vdc5W3EW6ggE4zbOHp2KtOI5kn4c+BLwiYj4y7FPIu1h0K3EBRdcsLaUriD1pltVwwXn8ZdfQ/zdG9DWwFXVAbSuBGE7XaoDp7MMTmUrLm0FyDoCwRKweej2JuBE2XMknckgCHw+Ih6c9CQRsQ/YB4Ppo9WTfbqUBn5nZbngHDcNro2Bq7KtsK7WHm28FAdOmwhOq1Vc2gyQdYwRPAlsk7RV0lnAtcD+kXP2Ax/RwGXAKxFxsphN9DngcET8Tg1pWbOc+pybbv1MGoQvk47U9lGy6lJsfU8KTp+4/+nGvsymza1iKgeCiHgDuBE4yGCw94GIeFbSDZJuKE47ALwIHAV+F/h3xfEdwL8Edkp6uvi5umqa1qJLA79VNTlwtVJBXiYdqe2jZNWlOHC6UhBqqvLRZoCsZR1BRBxgUNgPH7tz6O8APjbmcf+b8eMHrcilz7nJgauVCvIy6Uix9mjVpDhwOqlreFkTXVdtdk9nt/uoNdv6WakgL5OOFGuPVk2Kre9xXcOjZl35aLN7OsstJrpkVgOlTbV+VqvlrJaOFGuPKenqQHpqre/RWXXjzLry0eaUaAeChKU6zW4aVQtyrxeYrA+fj5QsB6dJGxo2UfloK0BmuftoV0za7RAGzemuFIhdrbWmLpXdb/uors9sap997z7aQWVmMkD6tb/UugH6ogsD6akVhGXV8ZntUovNg8UJW61P0tMox5t288CuSn0gPfc1IF2a+uxAkLAUZjKMk3JBm1Phk/oiyC4VhLPQhRbbMgeChA1Ps5uk6dpf6gVtToVPitMwh3WpIJyF1FtswxwIEnfNRXM8trCTz/zqe95U+xvePbSpgjj1gja3wmf58/HNve/nsYWdyQQB6FZBOAupt9iGORB0xGjrYNzuoU0Eg9QL2twLn5R0qSCchdRbbMM8a6hDUtg9NPVdWr0ALR1eA9KdGXMOBB3UZq089YI29X3fc9OVgjB3DgQd1GatvAu1vJT3fTdLkQNBB7VdK+96LS/FL0Zpg1tFtsyBoIO6UCtPWeoD3k1wq8iGORB01Eq1ctf0Vpb6gHcT3CqyYZ4+2jOpL/hKQRemNc569bZbRTbMgaBnUl/wlYLU53c3Ecy93sKGuWuoZ1zTKyflAe8mum3annBgaXGLoGdc0+u+JoJ56q0ia5ZbBD3jml73NTWYnXKryJpVS4tA0lWSjkg6KmlhzP2S9Nni/mckXVz2sTYd1/QmS3n77GFdGMy2fqncIpC0DrgduAJYAp6UtD8inhs6bRewrfi5FLgDuLTkY21Krum9WZfmzXudiDWtjq6hS4CjEfEigKT7gN3AcGG+G7g3Bl+Q/LikDZI2AltKPNassq7Nm3cwtybV0TU0Bxwbur1UHCtzTpnHAiBpj6RFSYunTp2qnGjLi2dTmU1WR4tAY45FyXPKPHZwMGIfsA9gfn5+7Dld4FW/7fBqYrPJ6mgRLAGbh25vAk6UPKfMY3vDq37b4wFYs8nqCARPAtskbZV0FnAtsH/knP3AR4rZQ5cBr0TEyZKP7Q2v+m2PZ1OZTVa5aygi3pB0I3AQWAfcHRHPSrqhuP9O4ABwNXAUeBX46EqPrZqmVLmful0egDUbr5YFZRFxgEFhP3zszqG/A/hY2cf2lfupy/NYillzvMVEg9xPXY7HUsya5UDQIPdTl+OxFLNmea+hhrmfenUeSzFrlgOBJcdjKWZvNstxM3cNWXI8lmJ2ulmPm7lFYMnxpmvd5dleszHrvbIcCCxJHkvpni7t8No1sx43c9eQmdXCs71mZ9bfPOgWwRpM2/x1c9ly4NleszPrbx50IJjStM1fN5fLyyVgNp3PpioufZ3tlcLnctbjZhrs/tAt8/Pzsbi42Mpz79j76NgP+9yG9Ty2sLPy+bkaDZgwqPH0bcHduHyKwd7rczMoZKZ9Xau8D318D/uWJ0mHImJ+9LjHCKY0bfPXzeVyculfHpfP5arYLLbSmPZ1rfI+9HHlfC6fS3cNTWna5m9fm8t1yyVgrpafur8+s+zrutz9Me6zutL/GdW32V65fC7dIpjStIudvDiqnFnPikhFmfzUWciUeV2HFytN+3/6LpfPpQPBlKZt/vaxuTwLuQTMcfkcVUch89BTx/92fGr0+2BHX9dx3R8rnZ+S5XxuXfgqO/Y+WvsOtbl8Lt01tAbTNn/71lyehVxWEw/nc7mQHp6uUUchMzrAufzl4JMGpFdqgcxiALsuTczIy+Vz6VlDZjVqc43Jav38fZvZ1tV0t2nSrCG3CCx7dRXGa6mh1tVaHDfNcdSkmv+sFyvNSi4DuU1wIKhRCgtPbDp1di/MamOwMp+r1fr5YfLYw0rdHyl/pj0jrz4OBDXxCuJuBsI6C+9Z1FDLfq5We47VavjjWiapf6a72pJJkWcN1SSXhSeTdPV7hussvGcx1bDs52ql51jrTLXUP9OekVefSoFA0jmSHpb0QvH77AnnXSXpiKSjkhaGjt8q6XlJz0j675I2VElPm3Lvr0y90JikzsJ7FlMNy36uJj33Z371PTy2sHNNhWMXPtPXXDTHYws7+ebe93PTlRdy68EjM5tK2mdVWwQLwCMRsQ14pLh9GknrgNuBXcB24DpJ24u7HwZ+LiJ+Hvgz4OaK6WlNLgtPJulCoTFOnYX3LGqoZT9XbT53CrraIk1F1TGC3cB7i7/vAf4H8Fsj51wCHI2IFwEk3Vc87rmI+MOh8x4H/nnF9LRmLf2VXexTn6SrA3d1zxOve83INJ+rNp+7Lmu9Jmb9DV59VzUQvD0iTgJExElJbxtzzhxwbOj2EnDpmPP+NXD/pCeStAfYA3DBBResOcGzMm2BkvpA3LS6PHCX8oK/Nhc0Nf3cVa6JSS3P4y+/xo69j3a6ktWEVQOBpK8B7xhz1y0ln2N0hTucvpgSSbcAbwCfn/RPImIfsA8GC8pKPnejpilQ+laDyWUFZhvaDFRNPvdq40wrfbYmtUih+5WsJqwaCCLifZPuk/RdSRuL1sBG4KUxpy0Bm4dubwJODP2P64EPAJdHF5c5r1FX+9RXknLN2tK3Uq1+tZbCuBbpsC5XsppQdbB4P3B98ff1wJfHnPMksE3SVklnAdcWj0PSVQzGFD4YEa9WTEundGkgzqwJkz7766RVZ6QND5ZP0uVK1qxVDQR7gSskvQBcUdxG0vmSDgBExBvAjcBB4DDwQEQ8Wzz+PwM/ATws6WlJd1ZMT2fksquhWVmTrokfTOgoGC3Yl6eSTgoGrmRNVmmwOCL+Arh8zPETwNVDtw8AB8ac9w+qPH+XuU/d7HSTrolJG+lNKti7PHGhLd5iokXuUzc73aRrYpqC3ZWs6TkQ2Gn6tLbB+mEtBbsrWdNxIJiRLhaofVvbYP3hgn22vOncDHR1uXtX9wsys2ocCGagqwVqH9c2mNnq3DU0A10tULu6X5A1o4vdnW3o4uvkFsEMdHWxmNc22CRd7e5sWldfJweCGehqgTq6lfGG9WfyY2f+CJ+8/2nv7565rnZ3Nq2rr5O7hmagy/OYl2dneAaRDetqd+eoWXfblH2dUus+yiYQNP3Cd326W992R7Vq+jB+1ETlpszrlGIlK4uuoa7227WpLzVAq0dXuzuHNdFtU+Z1SrH7KIsWgWu30+tDDdDqU0d3Z9vdIU1Ubsq8TilWsrIIBCm+8Knzxl02qkp3ZwrdIU1VblZ7nVKsZGXRNdTV6ZxtynkG0UNPHWfH3kfZuvDV3ue1KSl0h6TSvZVKOoZl0SJw7XZtcpxBlFNem7SWVnndXUmpzOZLJR3DsggEKb7wXZLTGEtOeW3StN0hswrIqczmSyUdy7IIBJDeC98lOY2x5JTXJk3bKndAblYWYwRWTU5jLDnltUmjY05zG9bz6Q+9e2Kh7oDcrGxaBLZ2OY2x5JTXpk3TKk9xZs2otqfD1smBwFbV5TGWaS/WLue1T1IPyH2bVKCIWPuDpXOA+4EtwLeAfxER/2/MeVcBtwHrgLsiYu/I/b8J3AqcFxHfW+155+fnY3Fxcc3ptjyMXqwwKExW6pKwdKRc496x99GxLZa5Det5bGFnCykqR9KhiJgfPV61RbAAPBIReyUtFLd/a+SJ1wG3A1cAS8CTkvZHxHPF/ZuL+75dMS02RsoX06x5wLHbUp7g0bcxjKqDxbuBe4q/7wGuGXPOJcDRiHgxIl4H7iset+w/AZ8C1t40sbFy32Opbxer1avKwsG+TSqoGgjeHhEnAYrfbxtzzhxwbOj2UnEMSR8EjkfEn6z2RJL2SFqUtHjq1KmKyc5DCqs529S3i9XqU7WSlOLq4CpW7RqS9DXgHWPuuqXkc2jMsZD0luJ//NMy/yQi9gH7YDBGUPK5s9bVvdHrUveAY19fpxxV7Tbs26SCVQNBRLxv0n2SvitpY0SclLQReGnMaUvA5qHbm4ATwN8HtgJ/Imn5+NclXRIR35kiDzZBV/dGr0udF2ufX6cc1dFtmPIYxrSqDhbvB64H9ha/vzzmnCeBbZK2AseBa4Ffi4hnGepKkvQtYL7MrCErp0yNuGsDqmuZDlpHPrr2OtnKurBOoUlVxwj2AldIeoHBzJ+9AJLOl3QAICLeAG4EDgKHgQeKIGAzVmY1Z5cGVNsc/O7S62Sr61sff1WVWgQR8RfA5WOOnwCuHrp9ADiwyv/aUiUtNl4X90afpM1a+Uqvk8cOuqdvffxVeWVx5lJfwTmszVr5pNfpV951nscOOqpPffxVedO5zE27GVib2pwOOul1+qPnT2U9Rdf6wS0C60zNqO3Wy7jX6ZP3Pz32XI8dWJe4RWCdkWLrxYvWrA/cIrBOSa310nYrxawODgRmFXj2ifWBA4FZRam1Usym5TECM7PMORCYmWXOgcDMLHMeI5jA2waYWS4cCMZoY8thBx4za4u7hsZo+pu9cv9KSTNrlwPBGE1vbpb7V0qaWbscCMZoetsA73VvZm1yIBij6S+t8H41ZtYmB4Ixmt7czN+WZGZt8qyhCZrcNsD71ZhZmxwIEuH9asysLe4aMjPLnAOBmVnmKgUCSedIeljSC8Xvsyecd5WkI5KOSloYue/Xi/uelfQfq6THzMymV7VFsAA8EhHbgEeK26eRtA64HdgFbAeuk7S9uO9XgN3Az0fEzwK/XTE9ZmY2paqBYDdwT/H3PcA1Y865BDgaES9GxOvAfcXjAP4tsDci/gYgIl6qmB4zM5tS1UDw9og4CVD8ftuYc+aAY0O3l4pjAD8N/BNJT0j6n5J+cdITSdojaVHS4qlTpyom28zMlq06fVTS14B3jLnrlpLPoTHHYuj5zwYuA34ReEDSOyMi3vSAiH3APoD5+fk33W9mZmuzaiCIiPdNuk/SdyVtjIiTkjYC47p2loDNQ7c3ASeG7nuwKPj/WNIPgXMBV/nNzBpStWtoP3B98ff1wJfHnPMksE3SVklnAdcWjwN4CNgJIOmngbOA71VMk5mZTaFqINgLXCHpBeCK4jaSzpd0ACAi3gBuBA4Ch4EHIuLZ4vF3A++U9A0Gg8jXj+sWMjOz2VEXy935+flYXFxsOxlmZp0i6VBEzI8e98piM7PMORCYmWXOgcDMLHMOBGZmmXMgMDPLnAOBmVnmHAjMzDLnQGBmljkHAjOzzDkQmJllzoHAzCxzDgRmZplzIDAzy5wDgZlZ5jq5DbWkU8Cfr/Hh55Lnl9/kmO8c8wx55jvHPMP0+f6piDhv9GAnA0EVkhbH7cfddznmO8c8Q575zjHPUF++3TVkZpY5BwIzs8zlGAj2tZ2AluSY7xzzDHnmO8c8Q035zm6MwMzMTpdji8DMzIY4EJiZZS6rQCDpKklHJB2VtNB2emZB0mZJfyTpsKRnJX28OH6OpIclvVD8PrvttNZN0jpJT0n6SnE7hzxvkPRFSc8X7/kv9T3fkj5ZfLa/IekLkn6sj3mWdLeklyR9Y+jYxHxKurko245IunKa58omEEhaB9wO7AK2A9dJ2t5uqmbiDeA3IuJngMuAjxX5XAAeiYhtwCPF7b75OHB46HYOeb4N+IOIeBfwCwzy39t8S5oD/j0wHxE/B6wDrqWfef6vwFUjx8bms7jGrwV+tnjMfynKvFKyCQTAJcDRiHgxIl4H7gN2t5ym2kXEyYj4evH3XzEoGOYY5PWe4rR7gGvaSeFsSNoEvB+4a+hw3/P8k8AvA58DiIjXI+Jlep5v4AxgvaQzgLcAJ+hhniPifwH/d+TwpHzuBu6LiL+JiG8CRxmUeaXkFAjmgGNDt5eKY70laQtwEfAE8PaIOAmDYAG8rb2UzcRngE8BPxw61vc8vxM4Bfxe0SV2l6S30uN8R8Rx4LeBbwMngVci4g/pcZ5HTMpnpfItp0CgMcd6O3dW0o8DXwI+ERF/2XZ6ZknSB4CXIuJQ22lp2BnAxcAdEXER8Nf0o0tkoqJPfDewFTgfeKukD7ebqiRUKt9yCgRLwOah25sYNCl7R9KZDILA5yPiweLwdyVtLO7fCLzUVvpmYAfwQUnfYtDlt1PS79PvPMPgM70UEU8Ut7/IIDD0Od/vA74ZEaci4vvAg8A/ot95HjYpn5XKt5wCwZPANklbJZ3FYGBlf8tpqp0kMegzPhwRvzN0137g+uLv64EvN522WYmImyNiU0RsYfC+PhoRH6bHeQaIiO8AxyRdWBy6HHiOfuf728Blkt5SfNYvZzAO1uc8D5uUz/3AtZJ+VNJWYBvwx6X/a0Rk8wNcDfwZ8H+AW9pOz4zy+I8ZNAmfAZ4ufq4G/h6DWQYvFL/PaTutM8r/e4GvFH/3Ps/Ae4DF4v1+CDi77/kG/gPwPPAN4L8BP9rHPANfYDAO8n0GNf5/s1I+gVuKsu0IsGua5/IWE2Zmmcupa8jMzMZwIDAzy5wDgZlZ5hwIzMwy50BgZpY5BwIzs8w5EJiZZe7/A2jj06v93vdkAAAAAElFTkSuQmCC%0A)\n",
    "\n",
    "#### Bresuch-Pagan test<a href=\"#Bresuch-Pagan-test\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   The null hypothesis (H0): Homoscedasticity (등분산성) is present\n",
    "-   The alternative hypothesis: (Ha): Homoscedasticity is not present\n",
    "\n",
    "In \\[43\\]:\n",
    "\n",
    "    from statsmodels.compat import lzip\n",
    "    import statsmodels.stats.api as sms\n",
    "\n",
    "    names = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\n",
    "\n",
    "    test = sms.het_breuschpagan(model.resid, model.model.exog)\n",
    "    lzip(names, test)\n",
    "\n",
    "    # p-value가 0.05 미만이 아니므로, 등분산성이 존재한다고 볼 수 있음\n",
    "    # 등분산성 가정을 만족하지 않으면 제곱항 추가, 변수 변환 등 고려가 필요\n",
    "\n",
    "Out\\[43\\]:\n",
    "\n",
    "    [('Lagrange multiplier statistic', 4.379630030338111),\n",
    "     ('p-value', 0.49614819719458125),\n",
    "     ('f-value', 0.8610826814043929),\n",
    "     ('f p-value', 0.5103673804157989)]\n",
    "\n",
    "### 잔차 정규성<a href=\"#잔차-정규성\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[44\\]:\n",
    "\n",
    "    # 정규 분포를 따르지 않음\n",
    "    from scipy import stats\n",
    "    k2, p = stats.normaltest(res)\n",
    "    print(p)\n",
    "\n",
    "    8.946146855803877e-07\n",
    "\n",
    "### 잔차 독립성<a href=\"#잔차-독립성\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   durbin_watson 테스트: 1.5와 2.5 사이에 있으면 정상\n",
    "\n",
    "In \\[45\\]:\n",
    "\n",
    "    from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "    #perform Durbin-Watson test\n",
    "    durbin_watson(model.resid)\n",
    "\n",
    "Out\\[45\\]:\n",
    "\n",
    "    2.298391259795813\n",
    "\n",
    "## 변수선택법<a href=\"#변수선택법\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "![image.png](data:image/png;base64,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)  \n",
    "\n",
    "-   K is the number of independent variables used\n",
    "-   L is the log-likelihood estimate (a.k.a. the likelihood that the\n",
    "    model could have produced your observed y-values).\n",
    "\n",
    "### 전진선택법<a href=\"#전진선택법\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[46\\]:\n",
    "\n",
    "    def processSubset(X,y,feature_set):\n",
    "        model = sm.OLS(y,X[list(feature_set)]) # Modeling\n",
    "        regr = model.fit() # model fitting\n",
    "        AIC = regr.aic # model's AIC\n",
    "        return {\"model\" : regr, \"AIC\" : AIC}\n",
    "\n",
    "In \\[47\\]:\n",
    "\n",
    "    def forward(X,y,predictors):\n",
    "        # predictor - 현재 선택되어있는 변수\n",
    "\n",
    "        remaining_predictors = [p for p in X.columns.difference(['const']) if p not in predictors]\n",
    "        results = []\n",
    "        for p in remaining_predictors :\n",
    "            results.append(processSubset(X=X,y=y,feature_set=predictors+[p]+['const']))\n",
    "\n",
    "        models = pd.DataFrame(results)\n",
    "        \n",
    "        # AIC가 가장 낮은 것을 선택\n",
    "        best_model = models.loc[models['AIC'].argmin()]\n",
    "        print(\"Selected predictors:\",best_model[\"model\"].model.exog_names,\"AIC: \",best_model[0].aic)\n",
    "        return best_model\n",
    "        \n",
    "    ### 전진선택법 모델\n",
    "    def forward_model(X,y):\n",
    "        Fmodels = pd.DataFrame(columns=[\"AIC\",\"model\"])\n",
    "        \n",
    "        # 미리 정의된 데이터 변수\n",
    "        predictors = []\n",
    "        \n",
    "        for i in range(1,len(X.columns.difference(['const']))+1):\n",
    "            Forward_result = forward(X=X,y=y,predictors=predictors)\n",
    "            if i > 1 :\n",
    "                if Forward_result[\"AIC\"] > Fmodel_before:\n",
    "                    break\n",
    "            Fmodels.loc[i] = Forward_result\n",
    "            predictors = Fmodels.loc[i][\"model\"].model.exog_names\n",
    "            Fmodel_before = Fmodels.loc[i][\"AIC\"]\n",
    "            predictors = [k for k in predictors if k != 'const']\n",
    "\n",
    "        return Fmodels['model'][len(Fmodels['model'])]\n",
    "\n",
    "In \\[48\\]:\n",
    "\n",
    "    # 상수가 반드시 포함되어 있어야 함: X = sm.add_constant(X) \n",
    "    print(forward_model(X,Y).summary())\n",
    "\n",
    "    Selected predictors: ['X1', 'const'] AIC:  27.990661502147674\n",
    "    Selected predictors: ['X1', 'X2', 'const'] AIC:  -428.4013516846708\n",
    "    Selected predictors: ['X1', 'X2', 'X4', 'const'] AIC:  -426.98009592720234\n",
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   R-squared:                       0.995\n",
    "    Model:                            OLS   Adj. R-squared:                  0.995\n",
    "    Method:                 Least Squares   F-statistic:                     9721.\n",
    "    Date:                Thu, 19 Aug 2021   Prob (F-statistic):          1.77e-112\n",
    "    Time:                        14:34:30   Log-Likelihood:                 217.20\n",
    "    No. Observations:                 100   AIC:                            -428.4\n",
    "    Df Residuals:                      97   BIC:                            -420.6\n",
    "    Df Model:                           2                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    X1             0.9947      0.010     99.345      0.000       0.975       1.015\n",
    "    X2             1.0109      0.010     96.949      0.000       0.990       1.032\n",
    "    const          0.0481      0.007      6.643      0.000       0.034       0.063\n",
    "    ==============================================================================\n",
    "    Omnibus:                       37.979   Durbin-Watson:                   2.315\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):                6.300\n",
    "    Skew:                          -0.048   Prob(JB):                       0.0428\n",
    "    Kurtosis:                       1.774   Cond. No.                         5.30\n",
    "    ==============================================================================\n",
    "\n",
    "    Notes:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "\n",
    "### 후진선택법<a href=\"#후진선택법\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[49\\]:\n",
    "\n",
    "    import itertools\n",
    "    def backward(X,y,predictors):\n",
    "        results = []\n",
    "        \n",
    "        # 데이터 변수들이 미리 정의된 predictors 조합 확인\n",
    "        \n",
    "        for combo in itertools.combinations(predictors, len(predictors) - 1):\n",
    "            results.append(processSubset(X=X,y=y,feature_set=list(combo)+['const']))\n",
    "        models = pd.DataFrame(results)\n",
    "        \n",
    "        # 가장 낮은 AIC를 가진 모델을 선택\n",
    "        best_model = models.loc[models['AIC'].argmin()]\n",
    "        \n",
    "        print(\"Selected predictors:\",best_model['model'].model.exog_names,' AIC:',best_model[0].aic)\n",
    "        return best_model\n",
    "        \n",
    "    def backward_model(X,y) :\n",
    "        Bmodels = pd.DataFrame(columns=[\"AIC\",\"model\"], index = range(1,len(X.columns)))\n",
    "        predictors = X.columns.difference(['const'])\n",
    "        Bmodel_before = processSubset(X,y,predictors)['AIC']\n",
    "        while (len(predictors) > 1):\n",
    "            Backward_result = backward(X=X, y= y,predictors=predictors)\n",
    "            if Backward_result['AIC'] > Bmodel_before :\n",
    "                break\n",
    "            Bmodels.loc[len(predictors) -1] = Backward_result\n",
    "            predictors = Bmodels.loc[len(predictors) - 1]['model'].model.exog_names\n",
    "            Bmodel_before = Backward_result[\"AIC\"]\n",
    "            predictors = [k for k in predictors if k != 'const']\n",
    "        return (Bmodels[\"model\"].dropna().iloc[0])\n",
    "\n",
    "In \\[50\\]:\n",
    "\n",
    "    # 상수가 반드시 포함되어 있어야 함: X = sm.add_constant(X) \n",
    "    print(backward_model(X,Y).summary())\n",
    "\n",
    "    Selected predictors: ['X1', 'X2', 'X4', 'X5', 'const']  AIC: -425.44443485780164\n",
    "    Selected predictors: ['X1', 'X2', 'X4', 'const']  AIC: -426.98009592720234\n",
    "    Selected predictors: ['X1', 'X2', 'const']  AIC: -428.4013516846708\n",
    "    Selected predictors: ['X1', 'const']  AIC: 27.990661502147674\n",
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   R-squared:                       0.995\n",
    "    Model:                            OLS   Adj. R-squared:                  0.995\n",
    "    Method:                 Least Squares   F-statistic:                     9721.\n",
    "    Date:                Thu, 19 Aug 2021   Prob (F-statistic):          1.77e-112\n",
    "    Time:                        14:34:30   Log-Likelihood:                 217.20\n",
    "    No. Observations:                 100   AIC:                            -428.4\n",
    "    Df Residuals:                      97   BIC:                            -420.6\n",
    "    Df Model:                           2                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    X1             0.9947      0.010     99.345      0.000       0.975       1.015\n",
    "    X2             1.0109      0.010     96.949      0.000       0.990       1.032\n",
    "    const          0.0481      0.007      6.643      0.000       0.034       0.063\n",
    "    ==============================================================================\n",
    "    Omnibus:                       37.979   Durbin-Watson:                   2.315\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):                6.300\n",
    "    Skew:                          -0.048   Prob(JB):                       0.0428\n",
    "    Kurtosis:                       1.774   Cond. No.                         5.30\n",
    "    ==============================================================================\n",
    "\n",
    "    Notes:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "\n",
    "### 단계적 선택법<a href=\"#단계적-선택법\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[51\\]:\n",
    "\n",
    "    def Stepwise_model(X,y):\n",
    "        Stepmodels = pd.DataFrame(columns = [\"AIC\",\"model\"])\n",
    "        predictors = []\n",
    "        Smodel_before = processSubset(X,y,predictors + ['const'])['AIC']\n",
    "        \n",
    "        # 변수 1~10개 0-9 -> 1-10\n",
    "        for i in range(1,len(X.columns.difference(['const']))+1):\n",
    "            Forward_result = forward(X=X,y=y,predictors = predictors) # constant added\n",
    "            Stepmodels.loc[i] = Forward_result\n",
    "            \n",
    "            predictors = Stepmodels.loc[i]['model'].model.exog_names\n",
    "            predictors = [k for k in predictors if k != 'const']\n",
    "            Backward_result = backward(X=X,y=y,predictors = predictors)\n",
    "            if Backward_result[\"AIC\"] < Forward_result[\"AIC\"]:\n",
    "                Stepmodels.loc[i] = Backward_result\n",
    "                predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n",
    "                Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
    "                predictors = [k for k in predictors if k != \"const\"]\n",
    "                print('backward')\n",
    "            if Stepmodels.loc[i][\"AIC\"] > Smodel_before:\n",
    "                break\n",
    "            else :\n",
    "                Smodel_before = Stepmodels.loc[i][\"AIC\"]\n",
    "        return Stepmodels[\"model\"][len(Stepmodels[\"model\"])]\n",
    "\n",
    "In \\[52\\]:\n",
    "\n",
    "    print(Stepwise_model(X,Y).summary())\n",
    "\n",
    "    Selected predictors: ['X1', 'const'] AIC:  27.990661502147674\n",
    "    Selected predictors: ['const']  AIC: 98.14853540392605\n",
    "    Selected predictors: ['X1', 'X2', 'const'] AIC:  -428.4013516846708\n",
    "    Selected predictors: ['X1', 'const']  AIC: 27.990661502147674\n",
    "    Selected predictors: ['X1', 'X2', 'X4', 'const'] AIC:  -426.98009592720234\n",
    "    Selected predictors: ['X1', 'X2', 'const']  AIC: -428.4013516846708\n",
    "    backward\n",
    "    Selected predictors: ['X1', 'X2', 'X4', 'const'] AIC:  -426.98009592720234\n",
    "    Selected predictors: ['X1', 'X2', 'const']  AIC: -428.4013516846708\n",
    "    backward\n",
    "    Selected predictors: ['X1', 'X2', 'X4', 'const'] AIC:  -426.98009592720234\n",
    "    Selected predictors: ['X1', 'X2', 'const']  AIC: -428.4013516846708\n",
    "    backward\n",
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   R-squared:                       0.995\n",
    "    Model:                            OLS   Adj. R-squared:                  0.995\n",
    "    Method:                 Least Squares   F-statistic:                     9721.\n",
    "    Date:                Thu, 19 Aug 2021   Prob (F-statistic):          1.77e-112\n",
    "    Time:                        14:34:30   Log-Likelihood:                 217.20\n",
    "    No. Observations:                 100   AIC:                            -428.4\n",
    "    Df Residuals:                      97   BIC:                            -420.6\n",
    "    Df Model:                           2                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    X1             0.9947      0.010     99.345      0.000       0.975       1.015\n",
    "    X2             1.0109      0.010     96.949      0.000       0.990       1.032\n",
    "    const          0.0481      0.007      6.643      0.000       0.034       0.063\n",
    "    ==============================================================================\n",
    "    Omnibus:                       37.979   Durbin-Watson:                   2.315\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):                6.300\n",
    "    Skew:                          -0.048   Prob(JB):                       0.0428\n",
    "    Kurtosis:                       1.774   Cond. No.                         5.30\n",
    "    ==============================================================================\n",
    "\n",
    "    Notes:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "\n",
    "## 다항 회귀<a href=\"#다항-회귀\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "class sklearn.preprocessing.PolynomialFeatures(degree=2, \\*,\n",
    "interaction_only=False, include_bias=True, order='C')\n",
    "\n",
    "For example, if an input sample is two dimensional and of the form \\[a,\n",
    "b\\], the degree-2 polynomial features are \\[1, a, b, a^2, ab, b^2\\].\n",
    "\n",
    "In \\[53\\]:\n",
    "\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    polynomial_features= PolynomialFeatures(degree=2, interaction_only = False)\n",
    "\n",
    "    A = pd.DataFrame({\"X1\":[1,2,3], \"X2\":[2,3,4]})\n",
    "    xp = polynomial_features.fit_transform(A)\n",
    "    xp.shape # bias, X1, X2, X1^2, X2^2, X1X2\n",
    "\n",
    "    # xp로 모델링하면 됨\n",
    "\n",
    "Out\\[53\\]:\n",
    "\n",
    "    (3, 6)\n",
    "\n",
    "# 로지스틱 회귀 분석<a href=\"#로지스틱-회귀-분석\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "## 데이터 준비<a href=\"#데이터-준비\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[54\\]:\n",
    "\n",
    "    from sklearn.datasets import make_classification\n",
    "    X,Y = make_classification(n_features = 5) \n",
    "\n",
    "## 기초<a href=\"#기초\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "-   Pseudo R-squ. is a substitute for R-squared. It also measures the\n",
    "    amount of outcome variable variance, which is explained by the\n",
    "    model. Pseudo R-squared can be interpreted in the same way as\n",
    "    R-squared; the higher the better, with a maximum of 1.\n",
    "\n",
    "-   LL-null and LLR p-value are equivalent to the F-statistic and\n",
    "    F-proba of linear regression, and are interpreted in the same manner\n",
    "    for comparing models. The higher the value for LL-null the better.\n",
    "    Low values for LLR p-value (\\<0.05) mean you can reject the null\n",
    "    hypothesis that the model based on the intercept (all coefficients\n",
    "    = 0) is better than the full model. Hence, our model is relevant.\n",
    "\n",
    "-   The z-statistic plays the same role as the t-statistic in the linear\n",
    "    regression output and equals the coefficient divided by its standard\n",
    "    error. The lower, the better.\n",
    "\n",
    "In \\[55\\]:\n",
    "\n",
    "    from statsmodels.discrete.discrete_model import Logit\n",
    "    model = Logit(Y, X) \n",
    "    model = model.fit()\n",
    "    print(model.summary())\n",
    "\n",
    "    Optimization terminated successfully.\n",
    "             Current function value: 0.138469\n",
    "             Iterations 9\n",
    "                               Logit Regression Results                           \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   No. Observations:                  100\n",
    "    Model:                          Logit   Df Residuals:                       96\n",
    "    Method:                           MLE   Df Model:                            3\n",
    "    Date:                Thu, 19 Aug 2021   Pseudo R-squ.:                  0.8002\n",
    "    Time:                        14:34:31   Log-Likelihood:                -13.847\n",
    "    converged:                       True   LL-Null:                       -69.315\n",
    "    Covariance Type:            nonrobust   LLR p-value:                 6.902e-24\n",
    "    ==============================================================================\n",
    "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    x1             0.6468        nan        nan        nan         nan         nan\n",
    "    x2             0.2020        nan        nan        nan         nan         nan\n",
    "    x3             4.5396    7.6e+06   5.97e-07      1.000   -1.49e+07    1.49e+07\n",
    "    x4             0.5489      0.546      1.005      0.315      -0.522       1.620\n",
    "    x5            -1.9118   1.26e+07  -1.52e-07      1.000   -2.47e+07    2.47e+07\n",
    "    =============================================================================="
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
