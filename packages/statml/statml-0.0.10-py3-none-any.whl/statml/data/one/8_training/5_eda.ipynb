{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc07c12",
   "metadata": {},
   "source": [
    "# 1번 머신러닝 문제 \n",
    "\n",
    "\n",
    "* 데이터 설명 : 신용카드 사기 탐지를 위한 예제 데이터이다. \n",
    "1. Time : 트랜잭션 시간, 의미 없음  \n",
    "2. V1~V28 :사용자 ID 및 민감한 기능(v1-v28)을 보호하기 위해 변환된 정보 \n",
    "3. Amount : 거래금액 \n",
    "4. Class : 0은 정상, 1은 이상치(신용카드 사기) \n",
    "\n",
    "## 1-1 EDA \n",
    "## 1-2 상관관계 시각화 전처리 필요함을 설명 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d451e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max.columns', 10) \n",
    "\n",
    "df= pd.read_csv(\"https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/27_problem1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f47e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c6828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## BoxPlot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## seaborn에 있는 BoxPlot을 그리기 위한 데이터 형변환 \n",
    "X = df.copy().drop(columns=['Time','Amount']) ## Amount는 Value값의 차이가 크므로제외 \n",
    "df_v1 = pd.melt(X ,var_name='col', value_name='value')\n",
    "df_v1\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "sns.boxplot(x = 'col', y = 'value', data = df_v1)\n",
    "plt.xticks(range(len(X.columns)), X.columns, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806c35b0",
   "metadata": {},
   "source": [
    "## 1-1 EDA 결과  \n",
    "## Amount 는 전처리가 필요해 보인다. \n",
    "## 이상치가 많아, 사기 데이터일 확률이 높아보인다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c271e",
   "metadata": {},
   "source": [
    "# 1-2 변수간 상관계수 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4354c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "# 그림 사이즈 지정\n",
    "fig, ax = plt.subplots( figsize=(20,15) )\n",
    "\n",
    "\n",
    "# 히트맵을 그린다\n",
    "sns.heatmap(df.corr(), \n",
    "            cmap = 'RdYlBu_r', \n",
    "            annot = False,   # 실제 값 표시 \n",
    "            linewidths=.5,  # 경계 구분\n",
    "           )  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab0e8b",
   "metadata": {},
   "source": [
    "## 데이터를 보니 0.7 이상의 값이 보이지 않는다. 차원축소를 해주는 것이 마땅한지 판단해야함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfeb28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60b309",
   "metadata": {},
   "source": [
    "# 1-2 전처리가 필요함 설명 \n",
    "\n",
    "\n",
    "* 이 데이터 세트는 25,446건의 거래 중 492건의 사기가 있는 거래를 나타냅니다.\n",
    "* 데이터 세트는 매우 불균형하며 긍정적인 클래스(사기)가 모든 거래의 1.93%를 차지합니다.\n",
    "\n",
    "1. 변수별로 이상치가 존재하나, 이상치를 찾아 처리하는 과제이므로 이상치를 제거하지는 않는다. \n",
    "2. 변수간의 상관성이 높아 전처리가 필요한 것은 없어보인다. \n",
    "3. Class 변수의 수를 보면 1이 현저히 작게 분포되어, 클래스의 불균형이 있음을 알 수 있다. \n",
    "  - 오버샘플링 or 언더 샘플링으로 처리가 필요해 보인다. \n",
    "4. Amount의 경우 다른 변수들과의 수치 값의 차이가 커 모델 사용시, scale을 적용할 필요가 있어 보인다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305926fe",
   "metadata": {},
   "source": [
    "# 2번문제 \n",
    "\n",
    "## 2-1 차원축소 방법 2가지 이상 비교하고 한가지 선택 \n",
    "## 2-2 추천한 한가지를 실제로 수행하고 선택한 이유 설명 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da733a91",
   "metadata": {},
   "source": [
    "## 2-1 문제 풀이 \n",
    "\n",
    "### 1. 요인분석(FA , Factor Analysis)\n",
    "\n",
    "- 요인분석은 수집된 많은 변수들이 있을 때, 유사한 항목들(공통차원)들끼리 묵어서 처리하는 기법이다. 이 때에는 독립변수와 종속변수의 개념이 없으며, 모든 변수들간의 관계를 분석함으로써 공통요인을 분석가의 판단으로 묵어서 처리할 수 있다. \n",
    "\n",
    "### 2. PCA  주성분분석\n",
    "* 주성분분석이란 데이터에 여러 변수들이 있을 때, 서로 상관성이 높은 변수들의 선형결합으로 이루어진 \"주성분\"이라는 새로운 변수를 만들어 변수들을 요약하고 축소하는 기법 \n",
    "<br>\n",
    "\n",
    "### <b> 선택 : 해당 데이터에 어울리는 분석은 PCA이다. </b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3d690",
   "metadata": {},
   "source": [
    "## 2-2 문제풀이 \n",
    "\n",
    "* 변수간의 스케일이 차이가 나면 스케일 큰 변수가 주성분에 영향을 많이 주기 때문에 주성분 분석 전에 변수를 표준화나 \n",
    "  정규화시켜주는 것이 좋지만, 해당 데이터의 v1 - v28은 값의 큰 차이가 없으므로 Amount만 -1~1 값으로 표준화 시키도록 하겠다. \n",
    "  \n",
    "* 파이썬에서 주성분분석을 수행할 수 있는 기본적인 함수는 sklearn의 PCA함수이다. \n",
    "* pca.explained_variance_ratio_를 통해서 또는 Scree Plot으로 주성분의 설명력을 확인 할 수 있다. \n",
    "* PCA함수는 공분산행렬의 고유벡터를 구하는 방법을 사용한다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. 추천한 이유 \n",
    "\n",
    "- 우선, 해당 데이터의 의미를 정확하게 이해하고 있지 못할 때에는 분석가의 주관적인 견해가 들어가는 요인분석은 사용하기가 어렵다. (미리 가정을 하고 분석하는 것에 어울림) \n",
    "- PCA는 데이터간의 선형 관계만을 가지고 판단하고, 주어진 데이터를 최대한 보존하여 저차원의 데이터를 얻을 수 있으므로 지금과 같은 가명처리 되어있는 변수에 적합한 기법이라 판단하였다. \n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1), copy=True)\n",
    "scaled_Amount = scaler.fit(df[[\"Amount\"]])\n",
    "scaled_Amount.transform(df[[\"Amount\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Scaled_Amount\"] = scaler.transform(df[[\"Amount\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec6ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#차원을 축소할 데이터만 추출\n",
    "features = df.columns.drop([\"Time\",\"Class\",\"Amount\"])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "## Scree Plot 으로 주성분 개수 정하는 방법 \n",
    "pca = PCA(n_components=29)\n",
    "pca_fit = pca.fit(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b2e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ==================== PCA Result Summary ===================\")\n",
    "print(\"\\n고유 값 : \\n\",pca.singular_values_)\n",
    "print(\"\\n분산 설명력: \\n\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869b1e1",
   "metadata": {},
   "source": [
    "분산설명력은 PCA를 사용했을 때, 주성분 1~29까지가 원본 데이터를 얼마나 설명할 수 있는가를 알려준다. 이를 시각화하여 원본데이터를 설명할 수 있는 최적의 주성분 개수를 파악할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree Plot \n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.plot(pca.explained_variance_ratio_,'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"주성분 3개로 축소할 시 분산설명력 : \",pca.explained_variance_ratio_[:3].sum()*100) \n",
    "print(\"주성분 9개로 축소할 시 분산설명력 : \",pca.explained_variance_ratio_[:9].sum()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c193d92",
   "metadata": {},
   "source": [
    "## 2. PCA 결과 \n",
    "\n",
    "- 해당 데이터는 앞서 주성분 그림만 보아서는 3개의 차원축소 값으로 사용하면 되어 보이지만 분산 설명력이 전체 데이터의 50%도 되지 않으므로 하나의 성분하나하나가 개별적 의미를 가진다고 볼 수 있다. \n",
    "\n",
    "- 그러므로 해당 데이터에서 차원의 축소는 적합하지 않아보인다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3dc9b",
   "metadata": {},
   "source": [
    "## 3.오버샘플링/언더샘플링\n",
    "\n",
    "3.1 오버샘플링과 언더샘플링 장단점 비교 및 선택 구현\n",
    "\n",
    "3.2 분류 분석에 대한 구현 및 알고리즘 2가지 (오버샘플링, 언더샘플링) 비교, 성능 측정\n",
    "\n",
    "3.3 현재까지 전처리한 데이터를 통해 분류 모델 수행 후 결과 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d72d13",
   "metadata": {},
   "source": [
    "## 답안\n",
    "\n",
    "### 오버샘플링  \n",
    "데이터 세트의 균형을 맞추기 위해 업 샘플링을 적용할 수 있다.\n",
    "대표적으로 랜덤 오버샘플링과 KNN을 이용한 SMOTE기법이 있다.\n",
    "1.  랜덤오버샘플링은 소수 클래스의 정보를 복제하여 다수 클래스의 크기에 맞추는 기법이며, 이는 오버피팅의 우려가 있다.\n",
    "2.  SMOTE는 KNN을 이용하여 최근접 이웃의 거리 계산을 통해 K개의 중간 지점의 데이터를 생성한다. 오버피팅을 막을 수 있으나, 새로운 데이터나 작은 데이터셋에서는 사용하기가 어렵다.\n",
    "\n",
    "### 언더샘플링\n",
    "언더 샘플링은 다수 데이터를 소수 데이터의 크기에 맞추는 기법이다.\n",
    "랜덤 샘플링을 통해 소수의 클래스로 만드는 과정에서 데이터의 손실이 발생하는 문제가 있을 수 있다. 소수 클래스의 크기가 너무 작을 경우 머신러닝 과정에서의 과소적합이 발생할 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "#### 데이터 셋이 크기 때문에 언더샘플링을 하게되면 데이터의 손실이 너무 크므로 오버샘플링 선택\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f6f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 원본 학습 시, 모델 : 컬럼이 많은 데이터셋에서도 적용가능한 랜덤포레스트 앙상블 모델 사용 \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=321)\n",
    "clf = RandomForestClassifier(random_state=321) \n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebbf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1번 오버샘플링 기법 - 랜덤오버샘플링 \n",
    "import imblearn. over_sampling\n",
    "from imblearn.over_sampling import  RandomOverSampler\n",
    "X_resampled, y_resampled = RandomOverSampler(random_state=0).fit_resample(X_train, y_train)\n",
    "\n",
    "clf_re = RandomForestClassifier(random_state=0) \n",
    "clf_re.fit(X_resampled,y_resampled)\n",
    "\n",
    "plot_roc_curve(clf_re, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2번 오버샘플링 - SMOTE \n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 모델설정\n",
    "sm = SMOTE( k_neighbors=5,random_state=2020)\n",
    "\n",
    "# train데이터를 넣어 복제함\n",
    "X_resampled_over, y_resampled_over = sm.fit_resample(X_train,y_train)\n",
    "\n",
    "clf_over = RandomForestClassifier(random_state=2020) \n",
    "clf_over.fit(X_resampled_over,y_resampled_over)\n",
    "\n",
    "plot_roc_curve(clf_over, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe33fa",
   "metadata": {},
   "source": [
    "## 3.3 현재까지 전처리한 데이터를 통해 모델 수행 후 결과 분석  \n",
    "\n",
    "* 오버샘플링을 2개의 기법 중, SMOTE기법의 AUC가 더 높은 것으로 보아, SMOTE 기법이 불균형한 소수 클래스의 특징을 잘 반영하여 데이터를 생성하였다고 볼 수 있다. \n",
    "\n",
    "* 하지만, 랜덤포래스트와 같은 경우 오버샘플링 기법이 없이도 0.96이라는 높은 예측력을 보이므로, 굳이 가상의 데이터를 만들어 예측력을 높이는 오버샘플링을 적용시킬 필요가 없는 데이터라고 할 수도 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2303e",
   "metadata": {},
   "source": [
    "# 4번 \n",
    "\n",
    "4.1 이상탐지 모델 2가지 이상 기술, 장/단점 설명\n",
    "\n",
    "4.2 앞에서 만든 데이터로 한 가지 이상탐지 모델을 구현하고, 3번에서 만든 모델과 비교\n",
    "\n",
    "4.3 데이터분석가 관점에서 3번에서 만든 모델과 4번에서 만든 모델 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331135c6",
   "metadata": {},
   "source": [
    "## 4.1 이상탐지 모델 2가지 이상 기술 장/단점 설명 \n",
    "\n",
    "1. DBSCAN \n",
    "- 클러스터가 더 낮은 밀도의 영역으로 분리된 공간의 밀도가 높은 영역이라는 가정하에 작동한다.\n",
    "- 데이터를 클러스터링하기 위해 DBSCAN 알고리즘은 데이터의 고밀도 영역과 저밀도 영역을 분리합니다. 거리와 군집당 최소 데이터 수를 사용하여 이상 데이터를 특이치로 분류합니다\n",
    "\n",
    "\n",
    "장/단점 \n",
    "- DBSCAM의 경우 주변 데이터에서 멀어지는 데이터의 특징을 찾아서 Class를 분류하기에 0 or 1이 아닌 특이점이라도 발견 할 수 있다. 아웃라이어를 탐색하는데에 적합하다. 하지만, 데이터의 특성을 잘 파악하여 군집 밀도와 최소 데이터 수 등 파라미터를 조정하는 것이 어려운 단점이 있다. \n",
    "\n",
    "\n",
    "2. SVM \n",
    "- 데이터를 선형모델을 분류하고자 할 때, 선형으로 완전히 분류할 수 없는 데이터의 차원을 올려주어 선으로 분류할 수 있게 커널트릭을 사용하여 데이터를 분류하는 방법이다. \n",
    "- SVM 분류 모델은 데이터 공간에 존재하는 경계로 표현되며, 데이터를 분류하는 여러 경계 중에서 가장 큰 마진으로 갖는 경계를 찾는다. 이 경계를 이용하여 이상치 Class1을 찾을 때에 이 모델을 사용할 수 있다. \n",
    "\n",
    "장/단점 \n",
    "- SVM의 경우는 비선형 분류에도 사용되어 오류 데이터에 대한 영향이 거의 없으므로, 과적합되는 경우가 적다. 즉, 오류가 많을 경우에는 해당 모델이 강점을 보일 수 있다. \n",
    "- 하지만, SVM은 학습속도가 느리고,  해석이 어렵고 복잡한 모델이라는 단점이있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a00c5",
   "metadata": {},
   "source": [
    "## 4.2 앞에서 만든 데이터로 한 가지 이상탐지 모델을 구현하고, 3번에서 만든 모델과 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f85a2d",
   "metadata": {},
   "source": [
    "- DBSCAN 이상탐지를 이용하기 위해서 적절한 epsilon 과 minPoints 값을 찾아야 한다. \n",
    "- epsilon 의 경우 K-distance 그래프를 사용하여 판단할 수 있다. \n",
    "- K 거리 그래프를 그리려면 데이터 세트의 모든 데이터 포인트에 대해 포인트와 가장 가까운 데이터 포인트 사이의 거리가 필요한데, sklearn.neighbors 에서 NearestNeighbors를 사용하여, 거리를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 예제 데이터가 너무 크므로, 학습 속도를 위해 랜덤샘플링 // 아마 실제 시험에서는 이정도로 큰 데이터는 제시하지 않을 것입니다. 샘플사이즈를 1/5 줄여서 학습시키겠습니다. \n",
    "features.drop(\"Scaled_Amount\")\n",
    "df_sample = df.sample(n=5000)\n",
    "df_sample = df_sample.drop(df_sample[df_sample[\"Class\"]==1].index)\n",
    "df_outlier = df[df[\"Class\"]==1].sample(n=60)\n",
    "df_sample_com = pd.concat([df_sample,df_outlier])\n",
    "X_sp = df_sample_com[features]\n",
    "y_sp = df_sample_com[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6abcfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np \n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(X_sp)\n",
    "distances, indices = nbrs.kneighbors(X_sp)\n",
    " \n",
    "#거리 변수는 데이터 세트의 모든 데이터 포인트에 대해 데이터 포인트와 가장 가까운 데이터 포인트 사이의 거리 배열을 포함\n",
    "\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting K-distance Graph\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(distances)\n",
    "plt.title('K-distance Graph',fontsize=20)\n",
    "plt.xlabel('Data Points sorted by distance',fontsize=14)\n",
    "plt.ylabel('Epsilon',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153f8b7",
   "metadata": {},
   "source": [
    "#엡실론 의 최적 값은 K-거리 그래프에서 최대 곡률 지점에 있으며 이 경우 5정도로 지정해주겠습니다. 이제 minPoints 의 값을 찾을 차례입니다 . minPoints 의 값 도 도메인 지식에 따라 다릅니다. 이번에는 minPoints를 10으로 사용합니다.\n",
    "\n",
    "모#델이 취하는 두 가지 가장 중요한 매개변수 값은 (i) 두 포인트 사이의 거리를 지정하는 esp 입니다. 즉, 클러스터의 일부로 간주되기 위해 데이터 포인트가 서로 얼마나 가까워야 하는지를 지정합니다. (ii) min_samples는 포인트가 클러스터에 있어야 하는 최소 이웃 수를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be756313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale data first\n",
    "\n",
    "db = DBSCAN(eps=6, min_samples=10).fit(X_sp)\n",
    "labels = db.labels_\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"Class\"] = y_sp.copy()\n",
    "data[\"labels\"] = labels\n",
    "data.loc[data[\"labels\"]==-1,'Class'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2195f5",
   "metadata": {},
   "source": [
    "## 60개의 이상치 중 53개를 분류해내었다. \n",
    " - DBSCAN의 놀라운 점은 데이터 세트에서 노이즈를 꽤 잘 분리한다. \n",
    " - 여기서 0은 양호한 클러스터이고 -1은 노이즈입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ffabb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[data[\"Class\"]==1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"labels\"]==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "## CM을 그리기 위해서 -1을 Class와 같이 1로 변경) \n",
    "data.loc[data.labels==-1, \"labels\"] = 1\n",
    "confusion_matrix(data.Class,data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d601890",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(data.Class,data.labels)\n",
    "print(\"정확도:\", accuracy)\n",
    "\n",
    "precision = metrics.precision_score(data.Class,data.labels)\n",
    "print(\"정밀도:\", precision)\n",
    "\n",
    "recall = metrics.recall_score(data.Class,data.labels)\n",
    "print(\"재현율:\", recall)\n",
    "\n",
    "f1 = metrics.f1_score(data.Class,data.labels)\n",
    "print(\"f1 점수:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0074a63",
   "metadata": {},
   "source": [
    "## 4.3 데이터분석가 관점에서 3번에서 만든 모델과 4번에서 만든 모델 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c7e60",
   "metadata": {},
   "source": [
    "# 결론\n",
    " - labels 가 -1이면서 Class가 0인 고객은 위험군으로 분류할 수 있다. \n",
    " - 4번 모델에서 labels 가 -1이면서 Class가 0인 고객은 위험군으로 분류할 수 있다. 분류의 정확도만 보자면 3번의 RandomForest 모델의 정확도가 훨씬 높아 사기를 당할 확률을 예상할 때에는 RandomForest모델이 좋아보인다. 하지만, 0과 1의 분류가 아닌 정상의 범주를 벗어나는 여러 케이스가 있을 경우에는 분류해내기 어렵다. 하지만 DBSCAN과 같은 거리기반 이상치 탐지는 정상으로 분류되지 않은 이상치를 가지고 위험군으로 관리할 수 있는 장점이 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8accaab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.444px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
