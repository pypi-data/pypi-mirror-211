{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리 호출\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn(\"once\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "# 선형모델을 추정하는 라이브러리\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "from patsy import dmatrices\n",
    "color = sns.color_palette()\n",
    "\n",
    "pd.set_option('display.float_format','{:,.4f}'.format) # 소수점 2번째 자리까지 표현\n",
    "pd.set_option('display.max_columns', None) # 모든 컬럼 표시\n",
    "pd.set_option('display.max_colwidth', -1) # 컬럼내용 전체 표시\n",
    "\n",
    "#Graph에 한글을 표시하기 위한 코드\n",
    "import matplotlib\n",
    "from matplotlib import font_manager, rc\n",
    "import platform\n",
    "# font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "font_name = font_manager.FontProperties(fname=\"/home/spa/.local/share/Trash/files/one/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\\\n",
    "    \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 통계분석 (사용 데이터 : Admission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) 종속변수인 chance_of_admit(입학 허가 확률)와 독립변수(GRE, TOEFL, Univ_Rating, SOP, LOR, CGPA)에 대해 피어슨 상관계수를 이용한 상관관계 분석을 수행하고 그래프를 이용하여 분석결과를 설명하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Admission.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'LOR ' : 'LOR'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = ['GRE', 'TOEFL', 'Univ_Rating', 'SOP', 'LOR', 'CGPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(columns = ['변수명', '상관계수', 'p-value'])\n",
    "corr_df['변수명'] = columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_list = []\n",
    "p_value_list = []\n",
    "for i in columns_list:\n",
    "    corr_list.append(stats.pearsonr(df['Chance_of_Admit'], df[i])[0])\n",
    "    p_value_list.append(stats.pearsonr(df['Chance_of_Admit'], df[i])[1])\n",
    "corr_df['상관계수'] = corr_list\n",
    "corr_df['p-value'] = p_value_list\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GRE 경우 상관계수는 0.80으로 입학허가확률과 강한 상관관계가 존재. p-value는 0.00로 유의수준 0.05하에서 두 변수간의 상관관계는 통계적으로 유의하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[['Chance_of_Admit', 'GRE', 'TOEFL', 'Univ_Rating', 'SOP', 'LOR', 'CGPA']].corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "cmap = sns.light_palette(\"darkgray\", as_cmap=True)\n",
    "sns.heatmap(df.corr(method = 'pearson'), annot=True, cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'GRE':'Research']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(df.drop(columns = ['Research']),\n",
    "               c = df['Chance_of_Admit'], # 점 색깔\n",
    "               marker = 'o', # 점 모양, default ‘.’\n",
    "               s = 10, # 점 크기\n",
    "               alpha = 0.8, # 투명도\n",
    "               figsize = (12, 8)\n",
    "              )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) GRE, TOEFL, Univ_Rating, SOP, LOR, CGPA, Research가 Chance_of_Admit에 \n",
    "###     영향을 미치는지 알아보는 회귀분석을 단계적 선택법을 사용하여 수행하고 결과를 해석하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS.from_formula('Chance_of_Admit ~ GRE + TOEFL + Univ_Rating + SOP + LOR + CGPA + Research', data = df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "#perform Durbin-Watson test\n",
    "durbin_watson(model.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Durbin-Watson 검정 결과값이 0.76로 0에 가깝기 때문에 독립성 가정을 만족한다고 보기 어렵다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = stats.shapiro(model.resid)\n",
    "shapiro_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Shapiro-Wilk 검정 결과 p-value가 0.09로 유의수준 0.05하에서 귀무가설을 기각한다. 따라서 adms 데이터는 정규분포를 따른다고 보기 어렵다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted = model.predict(df)\n",
    "residual = df['Chance_of_Admit'] - fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(fitted, residual, lowess=True, line_kws={'color': 'red'})\n",
    "plt.plot([fitted.min(), fitted.max()], [0, 0], '--', color='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그래프의 기울기가 직선 성향, 평균인 0을 중ㅇ심으로 고르게 분포(등분산성 만족)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "qqplot(model.resid, line='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 대각선에 벗어난 점이 많아 adms가 정규성을 만족한다고 보기 힘듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = stats.zscore(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(fitted, np.sqrt(np.abs(sr)), lowess=True, line_kws={'color': 'red'})\n",
    "plt.xlabel('Fitted_value')\n",
    "plt.ylabel('sqrt_resid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 빨간선의 기울기가 0에 가까워야 하지만 Fitted Value가 증가할수록 기울기가 줄어듬\n",
    "* 기울기가 0에서 떨어진 점이 있다면 표준화 잔차가 큼, y값 적합 잘 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "cd, _ = OLSInfluence(model).cooks_distance\n",
    "cd.sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 정형 데이터마이닝 (사용 데이터 : Titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) cabib, embarked변수의 값 중 \"\"로 처리된 값을 NA로 바꾸고 아래의 데이터 테이블을 보고 \n",
    "###     문자형, 범주형 변수들을 각각 character, factor형으로 변환하시오.\n",
    "###     또, 수치형 변수가 NA인 값을 중앙값으로 대체하고, 범주형 변수가 NA인 값을 최빈값으로 대체하고\n",
    "###     age변수를 아래의 표와 같이 구간화하여 age_1이라는 변수를 생성하고 추가하시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/titanic.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pclass'] = df['pclass'].astype('category')\n",
    "df['survived'] = df['survived'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_med = np.nanmedian(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['age'].fillna(value = age_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_med = np.nanmedian(df['fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fare'] = df['fare'].fillna(value = fare_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter as cc\n",
    "cnt = cc(df['cabin'])\n",
    "cnt.most_common()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cabin'] = df['cabin'].fillna(value = cnt.most_common()[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter as cc\n",
    "cnt = cc(df['embarked'])\n",
    "cnt.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embarked'] = df['embarked'].fillna(value = 'S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[((df['age'] >= 0) & (df['age'] < 10)), 'age_1'] = 0\n",
    "df.loc[((df['age'] >= 10) & (df['age'] < 20)), 'age_1'] = 1\n",
    "df.loc[((df['age'] >= 20) & (df['age'] < 30)), 'age_1'] = 2\n",
    "df.loc[((df['age'] >= 30) & (df['age'] < 40)), 'age_1'] = 3\n",
    "df.loc[((df['age'] >= 40) & (df['age'] < 50)), 'age_1'] = 4\n",
    "df.loc[((df['age'] >= 50) & (df['age'] < 60)), 'age_1'] = 5\n",
    "df.loc[((df['age'] >= 60) & (df['age'] < 70)), 'age_1'] = 6\n",
    "df.loc[((df['age'] >= 70) & (df['age'] < 80)), 'age_1'] = 7\n",
    "df.loc[((df['age'] >= 80) & (df['age'] < 90)), 'age_1'] = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) 전처리가 완료된 titanic 데이터를 train(70%), test(30%) 데이터로 분할하시오.\n",
    "###    (set.seed(12345)를 실행한 후 데이터를 분할하시오.) \n",
    "###    또, train 데이터로 종속변수인 survived(생존 여부)를 독립변수 pclass, sex, sibsp, parch, \n",
    "###    fare, embarked로 지정하여 예측하는 분류모델을 3개 이상 생성하고 test 데이터에 대한 \n",
    "###    예측값을 csv파일로 각각 제출하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['pclass', 'sex', 'sibsp', 'parch', 'fare', 'embarked']]\n",
    "y = df['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 6789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "lr_y_pred = lr.predict(X_test)\n",
    "pd.Series(lr_y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "rf_y_pred = rf.predict(X_test)\n",
    "pd.Series(rf_y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train)\n",
    "xgb_y_pred = xgb.predict(X_test)\n",
    "pd.Series(xgb_y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_test, lr_y_pred)\n",
    "print('오차 행렬:\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_test, rf_y_pred)\n",
    "print('오차 행렬:\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_test, xgb_y_pred)\n",
    "print('오차 행렬:\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, xgb_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) 생성된 3개의 분류모델에 대해 성과분석을 실시하여 정확도를 비교하여 설명하시오. \n",
    "###     또, ROC curve를 그리고 AUC값을 산출하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lr.decision_function(X_test))\n",
    "\n",
    "plt.plot(fpr, tpr, label = 'ROC 곡선')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR (재현율)')\n",
    "# 0 근처의 임계값을 찾습니다.\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize = 10,\n",
    "        label = '임계값 0', fillstyle = 'none', c = 'k', mew = 2)\n",
    "plt.legend(loc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lr.decision_function(X_test))\n",
    "\n",
    "plt.plot(fpr, tpr, label = 'ROC 곡선')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR (재현율)')\n",
    "# 0 근처의 임계값을 찾습니다.\n",
    "# close_zero = np.argmin(np.abs(thresholds))\n",
    "# plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize = 10,\n",
    "#         label = '임계값 0', fillstyle = 'none', c = 'k', mew = 2)\n",
    "plt.legend(loc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "lr_auc = roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])\n",
    "# svc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
    "print('lr의 AUC: {:.3f}'.format(lr_auc))\n",
    "# print('SVC의 AUC: {:.3f}'.format(svc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label = 'ROC 곡선')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR (재현율)')\n",
    "# 0 근처의 임계값을 찾습니다.\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize = 10,\n",
    "        label = '임계값 0', fillstyle = 'none', c = 'k', mew = 2)\n",
    "plt.legend(loc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "# svc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
    "print('랜덤 포레스트의 AUC: {:.3f}'.format(rf_auc))\n",
    "# print('SVC의 AUC: {:.3f}'.format(svc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, xgb.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label = 'ROC 곡선')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR (재현율)')\n",
    "# 0 근처의 임계값을 찾습니다.\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize = 10,\n",
    "        label = '임계값 0', fillstyle = 'none', c = 'k', mew = 2)\n",
    "plt.legend(loc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "xgb_auc = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1])\n",
    "# svc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
    "print('xgboost의 AUC: {:.3f}'.format(xgb_auc))\n",
    "# print('SVC의 AUC: {:.3f}'.format(svc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, xgb_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 비정형 데이터마이닝 (사용 데이터 : 문재인대통령 취임사)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ‘연설문.txt’ 데이터를 읽어온 뒤 숫자, 특수 문자 등을 제거하는 전처리 작업을 시행하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r', encoding = 'cp949') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_data('./data/연설문.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(x for x in np.array(text).reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(text):\n",
    "    repl = ''\n",
    "    pattern  = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "    text = re.sub(pattern = pattern, repl = repl, string = text)\n",
    "    pattern  = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text = re.sub(pattern = pattern, repl = repl, string = text)\n",
    "    pattern  = '([ㄱ-ㅎㅏ-ㅣ]+)'\n",
    "    text = re.sub(pattern = pattern, repl = repl, string = text)\n",
    "    pattern  = '<[^>]*>'\n",
    "    text = re.sub(pattern = pattern, repl = repl, string = text)\n",
    "    pattern = '[^\\w\\s]'\n",
    "    text = re.sub(pattern = pattern, repl = repl, string = text)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = cleansing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "nouns = komoran.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(columns = ['명사', '빈도'])\n",
    "freq = cnt.most_common(10)\n",
    "temp_list = []\n",
    "temp_list2= []\n",
    "for i in freq:\n",
    "    temp_list.append(i[0])\n",
    "    temp_list2.append(i[1])\n",
    "temp_df['명사'] = temp_list\n",
    "temp_df['빈도'] = temp_list2\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "sns.barplot(x = '명사', y = '빈도', data = temp_df)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "for i in cnt.most_common(20):\n",
    "    words[i[0]] = i[1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "wordcloud = WordCloud(font_path = \"/usr/share/fonts/NanumGothicCoding.ttf\", background_color='white',colormap = \"Accent_r\",\n",
    "                      width=1500, height=1000).generate_from_frequencies(words)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud = WordCloud(font_path = 'C:/Windows/Fonts/malgun.ttf', background_color='white',colormap = \"Accent_r\",\n",
    "#                       width=1500, height=1000).generate_from_frequencies(words)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
