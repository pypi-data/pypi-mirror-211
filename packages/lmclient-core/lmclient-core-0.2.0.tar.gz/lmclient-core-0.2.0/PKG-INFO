Metadata-Version: 2.1
Name: lmclient-core
Version: 0.2.0
Summary: LM Async Client, openai client, azure openai client ...
Home-page: https://github.com/wangyuxinwhy/lmclient
License: Apache-2.0
Keywords: lmclient,openai,azure,async
Author: wangyuxin
Author-email: wangyuxin@mokahr.com
Requires-Python: >=3.8,<4.0
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Requires-Dist: asyncer (==0.0.2)
Requires-Dist: diskcache (>=5.6.1,<6.0.0)
Requires-Dist: openai (>=0.27.7,<0.28.0)
Requires-Dist: typing-extensions (>=4.6.2,<5.0.0)
Project-URL: Repository, https://github.com/wangyuxinwhy/lmclient
Description-Content-Type: text/markdown

# lmclient

LM Async Client, OpenAI, Azure ...


## Install

```shell
pip install lmclient-core
```

## Usage

```python
from lmclient import LMClient, AzureCompletion, OpenAICompletion

openai_completion = OpenAICompletion(model='gpt-3.5-turbo')
# azure_completion = AzureCompletion()
client = LMClient(openai_completion, async_capacity=5, max_requests_per_minute=20)
prompts = [
    'Hello, my name is',
    'can you please tell me your name?',
    'i want to know your name',
    'what is your name?',
]
values = client.async_run(prompts=prompts)
print(values)
```

## Advanced Usage

```python
# limit max_requests_per_minute to 20
# limit async_capacity to 5 (max 5 async requests at the same time)
# use cache
# set error_mode to ignore (ignore or raise)

from lmclient import LMClient, OpenAICompletion
openai_completion = OpenAICompletion(model='gpt-3.5-turbo', max_requests_per_minute=20, async_capacity=5, cache_dir='openai_cache', error_mode='ignore')
```
